#!/usr/bin/env python3
"""
New-API Channel Proxy Updater
ä¸€ä¸ªå®šæ—¶ä»»åŠ¡ï¼Œç”¨äºå®šæœŸæ›´æ–°æŒ‡å®šæ¸ é“çš„ä»£ç†IP

ä»£ç†æ¥æºï¼šhttps://github.com/TopChina/proxy-list/blob/main/README.md

ä»£ç†æµ‹è¯•è¯´æ˜ï¼š
- ä»£ç†æ¯å°æ—¶æ›´æ–°ä¸€æ¬¡ç”¨æˆ·åï¼Œå¯†ç é»˜è®¤ä¸º1
- å¹¶å‘æµ‹è¯•å€™é€‰ä»£ç†ï¼Œä½¿ç”¨å¸¦è®¤è¯çš„ä»£ç† URL
- ä½¿ç”¨ TTFBï¼ˆé¦–å­—èŠ‚æ—¶é—´ï¼‰è¡¡é‡å»¶è¿Ÿï¼Œé¿å…ä¸‹è½½æ­£æ–‡å½±å“
- å¤š URL Ã— å¤šæ¬¡é‡‡æ ·ï¼ŒæŒ‰æˆåŠŸæ ·æœ¬å»æå€¼åå–å¹³å‡

ç¯å¢ƒå˜é‡ï¼š
- BASE_URL: New-APIå®ä¾‹çš„URL
- ADMIN_ID: ç®¡ç†å‘˜ç”¨æˆ·ID
- ADMIN_TOKEN: ç®¡ç†å‘˜è®¿é—®ä»¤ç‰Œ
- CHANNEL_IDS: éœ€è¦æ›´æ–°ä»£ç†çš„æ¸ é“IDåˆ—è¡¨
- PROXY_REGION: ä»£ç†åœ°åŒº (é»˜è®¤: é¦™æ¸¯)
- MAX_PROXY_TEST_COUNT: æœ€å¤§ä»£ç†æµ‹è¯•æ•°é‡ (é»˜è®¤: 5)
- MIN_SUCCESS_RATE: æœ€ä½æˆåŠŸç‡è¦æ±‚ (é»˜è®¤: 0.8)
- MAX_LATENCY_MS: æœ€å¤§å¯æ¥å—å»¶è¿Ÿ (é»˜è®¤: 5000ms)
- TEST_COUNT: æ¯ä¸ª URL çš„é‡‡æ ·æ¬¡æ•° (é»˜è®¤: 3)
- TEST_TIMEOUT: å•æ¬¡è¯·æ±‚è¶…æ—¶ç§’æ•° (é»˜è®¤: 10)
- TEST_CONCURRENCY: å¹¶å‘å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 5)
- TEST_URLS: è‡ªå®šä¹‰æµ‹è¯• URLï¼ˆé€—å·åˆ†éš”ï¼Œå¯ç•™ç©ºä½¿ç”¨é»˜è®¤204ç«¯ç‚¹ï¼‰
"""

import os
import json
import re
import requests
import logging
import sys
import warnings
import hashlib
from typing import List, Tuple, Optional, Dict, Any
from datetime import datetime
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import quote

# ç¦ç”¨ urllib3 çš„ InsecureRequestWarning è­¦å‘Š
warnings.filterwarnings('ignore', message='Unverified HTTPS request')
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('proxy_updater.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ç¼“å­˜æ–‡ä»¶è·¯å¾„
PROXY_CACHE_FILE = 'proxy_cache.json'


def get_cached_md5() -> Optional[str]:
    """
    è¯»å–ç¼“å­˜çš„MD5å€¼
    
    Returns:
        Optional[str]: ç¼“å­˜çš„MD5å€¼ï¼Œå¦‚æœç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥åˆ™è¿”å›None
    """
    try:
        if os.path.exists(PROXY_CACHE_FILE):
            with open(PROXY_CACHE_FILE, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                return cache_data.get('last_md5')
    except (json.JSONDecodeError, IOError) as e:
        logger.warning(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
    return None


def save_cached_md5(md5_value: str) -> None:
    """
    ä¿å­˜æ–°çš„MD5å€¼åˆ°ç¼“å­˜æ–‡ä»¶
    
    Args:
        md5_value: è¦ä¿å­˜çš„MD5å€¼
    """
    try:
        cache_data = {
            'last_md5': md5_value,
            'last_updated': datetime.now().isoformat()
        }
        with open(PROXY_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        logger.debug(f"å·²ä¿å­˜MD5ç¼“å­˜: {md5_value}")
    except IOError as e:
        logger.warning(f"ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")


def calculate_content_md5(content: str) -> str:
    """
    è®¡ç®—å†…å®¹çš„MD5å€¼
    
    Args:
        content: è¦è®¡ç®—MD5çš„å†…å®¹
    
    Returns:
        str: å†…å®¹çš„MD5å€¼
    """
    return hashlib.md5(content.encode('utf-8')).hexdigest()


def test_proxy_connectivity(proxy_ip: str, proxy_port: str, proxy_user: str, proxy_password: str = "1",
                          test_url: str = "http://www.gstatic.com/generate_204", timeout: int = 10) -> dict:
    """
    æµ‹è¯•ä»£ç†çš„è¿é€šæ€§å’Œå»¶è¿Ÿ
    
    Args:
        proxy_ip: ä»£ç†IPåœ°å€
        proxy_port: ä»£ç†ç«¯å£
        proxy_user: ä»£ç†ç”¨æˆ·å
        proxy_password: ä»£ç†å¯†ç  (é»˜è®¤ä¸º1)
        test_url: æµ‹è¯•URL (é»˜è®¤ http://www.gstatic.com/generate_204)
        timeout: è¶…æ—¶æ—¶é—´ (ç§’)
    
    Returns:
        dict: åŒ…å«è¿é€šæ€§å’Œå»¶è¿Ÿä¿¡æ¯çš„å­—å…¸
    """
    try:
        # æ„å»ºä»£ç†URLï¼ˆå¸¦è®¤è¯ä¿¡æ¯ï¼‰
        encoded_user = quote(proxy_user)
        proxy_url = f"http://{encoded_user}:{proxy_password}@{proxy_ip}:{proxy_port}"
        
        # è®¾ç½®ä»£ç†
        proxies = {
            'http': proxy_url,
            'https': proxy_url
        }
        
        # ä½¿ç”¨ User-Agentï¼›ä¸å†æ‰‹å·¥è®¾ç½® Proxy-Authorizationï¼Œäº¤ç»™ requests å¤„ç†
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        logger.debug(f"æ­£åœ¨æµ‹è¯•ä»£ç†: {proxy_ip}:{proxy_port}")
        
        # å‘é€æµ‹è¯•è¯·æ±‚ï¼ˆstream=Trueï¼Œä»…æµ‹åˆ°é¦–å­—èŠ‚/é¦–åŒ…æ—¶é—´ï¼‰
        response = requests.get(
            test_url,
            proxies=proxies,
            headers=headers,
            timeout=timeout,
            verify=False,
            stream=True,
            allow_redirects=False
        )
        latency_ms = response.elapsed.total_seconds() * 1000 if response.elapsed else None
        # å…³é—­è¿æ¥ï¼Œé¿å…ä¸‹è½½æ­£æ–‡
        response.close()

        if 200 <= response.status_code < 400 and latency_ms is not None:
            logger.debug(f"âœ… ä»£ç† {proxy_ip}:{proxy_port} æµ‹è¯•æˆåŠŸï¼ŒTTFB: {latency_ms:.1f}ms")
            return {
                'is_working': True,
                'latency_ms': latency_ms,
                'status_code': response.status_code,
                'test_url': test_url,
                'error': None
            }
        else:
            logger.debug(f"âŒ ä»£ç† {proxy_ip}:{proxy_port} æµ‹è¯•å¤±è´¥: HTTP {response.status_code}")
            return {
                'is_working': False,
                'latency_ms': latency_ms,
                'status_code': response.status_code,
                'test_url': test_url,
                'error': f"HTTP {response.status_code}"
            }
            
    except requests.exceptions.Timeout:
        logger.debug(f"âŒ ä»£ç† {proxy_ip}:{proxy_port} æµ‹è¯•è¶…æ—¶")
        return {
            'is_working': False,
            'latency_ms': None,
            'status_code': None,
            'test_url': test_url,
            'error': "Timeout"
        }
    except requests.exceptions.ProxyError:
        logger.debug(f"âŒ ä»£ç† {proxy_ip}:{proxy_port} ä»£ç†é”™è¯¯")
        return {
            'is_working': False,
            'latency_ms': None,
            'status_code': None,
            'test_url': test_url,
            'error': "Proxy Error"
        }
    except requests.exceptions.ConnectionError:
        logger.debug(f"âŒ ä»£ç† {proxy_ip}:{proxy_port} è¿æ¥é”™è¯¯")
        return {
            'is_working': False,
            'latency_ms': None,
            'status_code': None,
            'test_url': test_url,
            'error': "Connection Error"
        }
    except Exception as e:
        logger.debug(f"âŒ ä»£ç† {proxy_ip}:{proxy_port} æµ‹è¯•å¼‚å¸¸: {str(e)}")
        return {
            'is_working': False,
            'latency_ms': None,
            'status_code': None,
            'test_url': test_url,
            'error': str(e)
        }


def measure_proxy_latency(
    proxy_ip: str,
    proxy_port: str,
    proxy_user: str,
    proxy_password: str = "1",
    test_urls: List[str] = None,
    test_count: int = 3,
    timeout: int = 10,
) -> dict:
    """
    ç²¾ç¡®æµ‹é‡ä»£ç†å»¶è¿Ÿï¼Œä½¿ç”¨å¤šä¸ªURLå’Œå¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
    
    Args:
        proxy_ip: ä»£ç†IPåœ°å€
        proxy_port: ä»£ç†ç«¯å£
        proxy_user: ä»£ç†ç”¨æˆ·å
        proxy_password: ä»£ç†å¯†ç  (é»˜è®¤ä¸º1)
        test_urls: æµ‹è¯•URLåˆ—è¡¨ (é»˜è®¤ä½¿ç”¨å†…ç½®çš„å¿«é€Ÿå“åº”ç«™ç‚¹)
        test_count: æ¯ä¸ªURLæµ‹è¯•æ¬¡æ•° (é»˜è®¤ä¸º3)
        timeout: è¶…æ—¶æ—¶é—´ (ç§’)
    
    Returns:
        dict: åŒ…å«è¯¦ç»†å»¶è¿Ÿä¿¡æ¯çš„å­—å…¸
    """
    if test_urls is None:
        # é€‰æ‹©æå°å“åº”æˆ– 204 çš„æ¢æ´»ç«¯ç‚¹ï¼Œå‡å°‘ä¸‹è½½æ—¶é—´å½±å“
        test_urls = [
            "http://www.gstatic.com/generate_204",
            "https://www.gstatic.com/generate_204",
            "http://cp.cloudflare.com/generate_204",
        ]
    
    all_results = []
    working_tests = 0
    
    logger.debug(f"å¼€å§‹ç²¾ç¡®æµ‹é‡ä»£ç†å»¶è¿Ÿ: {proxy_ip}:{proxy_port}")
    
    test_details: List[Dict[str, Any]] = []

    for test_url in test_urls:
        url_results: List[Dict[str, Any]] = []

        for i in range(test_count):
            result = test_proxy_connectivity(
                proxy_ip, proxy_port, proxy_user, proxy_password, test_url, timeout
            )
            url_results.append(result)

            if result['is_working'] and result['latency_ms'] is not None:
                working_tests += 1
                all_results.append(result['latency_ms'])

            # æ·»åŠ çŸ­æš‚å»¶è¿Ÿé¿å…é¢‘ç¹è¯·æ±‚
            if i < test_count - 1:
                time.sleep(0.2)

        test_details.append({
            'url': test_url,
            'results': url_results,
        })
    
    if not working_tests:
        logger.debug(f"âŒ ä»£ç† {proxy_ip}:{proxy_port} æ‰€æœ‰æµ‹è¯•å‡å¤±è´¥")
        return {
            'proxy': f"{proxy_ip}:{proxy_port}",
            'user': proxy_user,
            'is_working': False,
            'avg_latency_ms': None,
            'min_latency_ms': None,
            'max_latency_ms': None,
            'success_rate': 0.0,
            'test_count': len(test_urls) * test_count,
            'working_tests': 0,
            'test_details': test_details,
        }
    
    # å»æ‰æœ€é«˜å’Œæœ€ä½å»¶è¿Ÿåè®¡ç®—å¹³å‡å€¼
    valid_latencies = [latency for latency in all_results if latency is not None]
    filtered_latencies: List[float] = []
    if len(valid_latencies) >= 5:
        valid_latencies.sort()
        trim = max(1, int(len(valid_latencies) * 0.1))
        filtered_latencies = valid_latencies[trim: len(valid_latencies) - trim]
    else:
        filtered_latencies = valid_latencies

    avg_latency = (
        sum(filtered_latencies) / len(filtered_latencies)
        if filtered_latencies else None
    )
    min_latency = min(valid_latencies) if valid_latencies else None
    max_latency = max(valid_latencies) if valid_latencies else None
    success_rate = working_tests / (len(test_urls) * test_count)
    
    if avg_latency is not None:
        logger.debug(
            f"âœ… ä»£ç† {proxy_ip}:{proxy_port} å»¶è¿Ÿæµ‹è¯•å®Œæˆ: å¹³å‡={avg_latency:.1f}ms, æˆåŠŸç‡={success_rate:.1%}"
        )
    else:
        logger.debug(
            f"âš ï¸ ä»£ç† {proxy_ip}:{proxy_port} å»¶è¿Ÿæµ‹è¯•å®Œæˆ: æ— æœ‰æ•ˆæ ·æœ¬, æˆåŠŸç‡={success_rate:.1%}"
        )
    
    return {
        'proxy': f"{proxy_ip}:{proxy_port}",
        'user': proxy_user,
        'is_working': True,
        'avg_latency_ms': avg_latency,
        'min_latency_ms': min_latency,
        'max_latency_ms': max_latency,
        'success_rate': success_rate,
        'test_count': len(test_urls) * test_count,
        'working_tests': working_tests,
        'test_details': test_details
    }


def _measure_single_proxy(
    ip_port: str,
    user: str,
    test_urls: Optional[List[str]],
    test_count: int,
    timeout: int,
) -> dict:
    try:
        proxy_ip, proxy_port = ip_port.split(":")
    except ValueError:
        return {
            'proxy': ip_port,
            'user': user,
            'is_working': False,
            'avg_latency_ms': None,
            'min_latency_ms': None,
            'max_latency_ms': None,
            'success_rate': 0.0,
            'test_count': 0,
            'working_tests': 0,
            'test_details': [],
            'error': 'Invalid ip:port'
        }

    return measure_proxy_latency(
        proxy_ip, proxy_port, user,
        test_urls=test_urls,
        test_count=test_count,
        timeout=timeout,
    )


def find_best_proxy_by_latency(
    proxies: List[Tuple[str, str]],
    max_test_count: int = 5,
    min_success_rate: float = 0.8,
    max_latency_ms: int = 5000,
    test_urls: Optional[List[str]] = None,
    test_count: int = 3,
    timeout: int = 10,
    concurrency: int = 5,
) -> Optional[dict]:
    """
    éå†ä»£ç†åˆ—è¡¨ï¼Œæ‰¾åˆ°å»¶è¿Ÿæœ€ä½çš„å¯ç”¨ä»£ç†
    
    Args:
        proxies: ä»£ç†åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(ip_port, user)å…ƒç»„
        max_test_count: æœ€å¤§æµ‹è¯•æ•°é‡ (é»˜è®¤ä¸º5)
        min_success_rate: æœ€ä½æˆåŠŸç‡è¦æ±‚ (é»˜è®¤ä¸º0.8)
        max_latency_ms: æœ€å¤§å¯æ¥å—å»¶è¿Ÿ (é»˜è®¤ä¸º5000ms)
    
    Returns:
        Optional[dict]: æœ€ä½³ä»£ç†çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
    """
    logger.info(f"å¼€å§‹æµ‹è¯•ä»£ç†å»¶è¿Ÿï¼Œå…± {len(proxies)} ä¸ªä»£ç†ï¼Œæœ€å¤šæµ‹è¯• {max_test_count} ä¸ª")
    
    tested_proxies = []

    # æˆªå–åˆ°æœ€å¤š max_test_count ä¸ªå€™é€‰
    candidates = proxies[:max_test_count]

    logger.info(f"å¹¶å‘æµ‹è¯• {len(candidates)} ä¸ªå€™é€‰ä»£ç†ï¼Œæœ€å¤§å¹¶å‘={concurrency}")

    with ThreadPoolExecutor(max_workers=max(1, concurrency)) as executor:
        future_map = {
            executor.submit(
                _measure_single_proxy,
                ip_port,
                user,
                test_urls,
                test_count,
                timeout,
            ): (ip_port, user)
            for (ip_port, user) in candidates
        }

        for future in as_completed(future_map):
            ip_port, user = future_map[future]
            try:
                result = future.result()
            except Exception as e:
                logger.warning(f"ä»£ç† {ip_port} æµ‹è¯•å¼‚å¸¸: {e}")
                result = {
                    'proxy': ip_port,
                    'user': user,
                    'is_working': False,
                    'avg_latency_ms': None,
                    'min_latency_ms': None,
                    'max_latency_ms': None,
                    'success_rate': 0.0,
                    'test_count': 0,
                    'working_tests': 0,
                    'test_details': [],
                    'error': str(e)
                }

            tested_proxies.append(result)
            if result['is_working'] and result['avg_latency_ms'] is not None:
                logger.info(
                    f"âœ… ä»£ç† {ip_port} æµ‹è¯•å®Œæˆ: å¹³å‡å»¶è¿Ÿ={result['avg_latency_ms']:.1f}ms, æˆåŠŸç‡={result['success_rate']:.1%}"
                )
            else:
                logger.info(f"âŒ ä»£ç† {ip_port} ä¸å¯ç”¨")
    
    # ç­›é€‰å¯ç”¨ä»£ç†
    working_proxies = [
        proxy for proxy in tested_proxies
        if (
            proxy.get('is_working')
            and proxy.get('success_rate', 0) >= min_success_rate
            and proxy.get('avg_latency_ms') is not None
            and proxy.get('avg_latency_ms') <= max_latency_ms
        )
    ]
    
    if not working_proxies:
        logger.warning(f"âŒ åœ¨ {len(tested_proxies)} ä¸ªæµ‹è¯•çš„ä»£ç†ä¸­æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä»£ç†")
        return None
    
    # æŒ‰å»¶è¿Ÿæ’åºï¼Œé€‰æ‹©æœ€ä½³ä»£ç†
    best_proxy = min(working_proxies, key=lambda x: x['avg_latency_ms'])
    
    logger.info(f"ğŸ¯ é€‰æ‹©æœ€ä½³ä»£ç†: {best_proxy['proxy']}")
    logger.info(f"   - å¹³å‡å»¶è¿Ÿ: {best_proxy['avg_latency_ms']:.1f}ms")
    logger.info(f"   - æœ€å°å»¶è¿Ÿ: {best_proxy['min_latency_ms']:.1f}ms")
    logger.info(f"   - æœ€å¤§å»¶è¿Ÿ: {best_proxy['max_latency_ms']:.1f}ms")
    logger.info(f"   - æˆåŠŸç‡: {best_proxy['success_rate']:.1%}")
    logger.info(f"   - æµ‹è¯•æ¬¡æ•°: {best_proxy['working_tests']}/{best_proxy['test_count']}")
    
    return best_proxy


def extract_proxies_by_region(markdown_text: str, region: str) -> List[Tuple[str, str]]:
    """
    ä»Markdownæ–‡æœ¬ä¸­æå–æŒ‡å®šåœ°åŒºçš„ä»£ç†
    
    Args:
        markdown_text: åŒ…å«ä»£ç†åˆ—è¡¨çš„Markdownæ–‡æœ¬
        region: ç›®æ ‡åœ°åŒº
    
    Returns:
        åŒ…å«(ip_port, user)å…ƒç»„çš„åˆ—è¡¨
    """
    proxies: List[Tuple[str, str]] = []
    lines = markdown_text.split('\n')
    ip_port_regex = re.compile(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+$')
    
    for line in lines:
        # æ£€æŸ¥æ˜¯å¦ä¸ºè¡¨æ ¼æ•°æ®è¡Œ
        if line.startswith('|') and not line.startswith('|---'):
            # åˆ†å‰²åˆ—å¹¶å»é™¤é¦–å°¾ç©ºæ ¼
            columns = [col.strip() for col in line.split('|') if col.strip()]
            
            if len(columns) >= 3:
                ip_port = columns[0]
                country = columns[1]
                user = columns[2]
                
                # ç­›é€‰æŒ‡å®šåœ°åŒºä¸”IP:ç«¯å£æ ¼å¼æ­£ç¡®çš„ä»£ç†
                if country == region and ip_port_regex.match(ip_port):
                    proxies.append((ip_port, user))
    
    return proxies


def update_channel_proxy(base_url: str, admin_id: str, admin_token: str, channel_ids: List[int], proxy_url: str) -> None:
    """
    æ›´æ–°æŒ‡å®šæ¸ é“çš„ä»£ç†è®¾ç½®
    
    Args:
        base_url: New-APIåŸºç¡€URL
        admin_id: ç®¡ç†å‘˜ID
        admin_token: ç®¡ç†å‘˜ä»¤ç‰Œ
        channel_ids: æ¸ é“IDåˆ—è¡¨
        proxy_url: æ–°çš„ä»£ç†URL
    """
    api_url = f"{base_url.rstrip('/')}/api/channel/"
    
    logger.info(f"æ­£åœ¨æ›´æ–° {len(channel_ids)} ä¸ªæ¸ é“çš„ä»£ç†...")
    logger.debug(f"updateChannelProxy: proxyUrl={proxy_url}")
    logger.debug(f"updateChannelProxy: channelIds={channel_ids}")
    logger.debug(f"updateChannelProxy: baseUrl={base_url}")
    
    headers = {
        'New-Api-User': admin_id,
        'Authorization': f'Bearer {admin_token}',
        'Content-Type': 'application/json',
    }
    
    for channel_id in channel_ids:
        update_data = {
            'id': channel_id,
            'setting': json.dumps({'proxy': proxy_url}),
        }
        
        logger.info(f"æ­£åœ¨æ›´æ–°æ¸ é“ {channel_id} çš„ä»£ç†...")
        logger.debug(f"updateChannelProxy: updateData={json.dumps(update_data)}")
        logger.debug(f"updateChannelProxy: headers.New-Api-User={admin_id}")
        logger.debug(f"updateChannelProxy: headers.Authorization=Bearer {admin_token[:10]}...")
        logger.debug(f"updateChannelProxy: body={json.dumps(update_data)}")
        
        try:
            response = requests.put(
                api_url,
                headers=headers,
                json=update_data,
                timeout=30
            )
            
            if response.ok:
                logger.info(f"âœ… æ¸ é“ {channel_id} ä»£ç†æ›´æ–°æˆåŠŸï¼")
            else:
                error_text = response.text
                logger.error(f"âŒ æ¸ é“ {channel_id} ä»£ç†æ›´æ–°å¤±è´¥ï¼šHTTP {response.status_code}: {error_text}")
                raise Exception(f"ä»£ç†æ›´æ–°å¤±è´¥ï¼šHTTP {response.status_code}: {error_text}")
                
        except requests.RequestException as e:
            logger.error(f"âŒ æ›´æ–°æ¸ é“ {channel_id} æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯ï¼š{str(e)}")
            raise


def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œä»£ç†æ›´æ–°ä»»åŠ¡"""
    logger.info("å¼€å§‹æ‰§è¡Œä»£ç†æ›´æ–°ä»»åŠ¡...")
    
    # æ­¥éª¤1: æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_env_vars = {
        'BASE_URL': os.getenv('BASE_URL'),
        'ADMIN_ID': os.getenv('ADMIN_ID'),
        'ADMIN_TOKEN': os.getenv('ADMIN_TOKEN'),
        'CHANNEL_IDS': os.getenv('CHANNEL_IDS')
    }
    
    missing_vars = [var for var, value in required_env_vars.items() if not value]
    if missing_vars:
        logger.error(f"é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        sys.exit(1)
    
    try:
        # è§£ææ¸ é“ID
        channel_ids = json.loads(required_env_vars['CHANNEL_IDS'])
        if not isinstance(channel_ids, list):
            raise ValueError("CHANNEL_IDS å¿…é¡»æ˜¯æ•°ç»„æ ¼å¼")
        
        # æ­¥éª¤2: ä»GitHubè·å–ä»£ç†åˆ—è¡¨
        logger.info("æ­£åœ¨ä»GitHubè·å–ä»£ç†åˆ—è¡¨...")
        proxy_list_url = "https://raw.githubusercontent.com/TopChina/proxy-list/refs/heads/main/README.md"
        
        try:
            response = requests.get(proxy_list_url, timeout=30)
            response.raise_for_status()
            markdown_text = response.text
            logger.info("âœ… æˆåŠŸè·å–ä»£ç†åˆ—è¡¨Markdownæ–‡ä»¶")
            
            # æ£€æŸ¥å†…å®¹MD5æ˜¯å¦ä¸ç¼“å­˜ç›¸åŒ
            current_md5 = calculate_content_md5(markdown_text)
            cached_md5 = get_cached_md5()
            
            if cached_md5 == current_md5:
                logger.info("ğŸ”„ ä»£ç†åˆ—è¡¨å†…å®¹æœªå˜åŒ–ï¼Œè·³è¿‡æœ¬æ¬¡æ‰§è¡Œ")
                return
            else:
                logger.info("ğŸ“ ä»£ç†åˆ—è¡¨å†…å®¹æœ‰æ›´æ–°ï¼Œç»§ç»­æ‰§è¡Œ")
        except requests.RequestException as e:
            logger.error(f"âŒ è·å–ä»£ç†åˆ—è¡¨å¤±è´¥ï¼š{str(e)}")
            return
        
        # æ­¥éª¤3: è§£æä»£ç†åˆ—è¡¨
        region = os.getenv('PROXY_REGION', 'é¦™æ¸¯')
        logger.info(f"æ­£åœ¨æŸ¥æ‰¾ {region} åœ°åŒºçš„ä»£ç†...")
        
        proxies = extract_proxies_by_region(markdown_text, region)
        
        if not proxies:
            logger.warning(f"åœ¨åˆ—è¡¨ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ {region} ä»£ç†ã€‚ä»»åŠ¡ç»“æŸã€‚")
            return
        
        logger.info(f"âœ… æ‰¾åˆ°äº† {len(proxies)} ä¸ª {region} ä»£ç†")
        
        # æ­¥éª¤4: æµ‹è¯•ä»£ç†å»¶è¿Ÿå¹¶æ‰¾åˆ°æœ€ä½³ä»£ç†ï¼ˆæ”¯æŒå‚æ•°è°ƒä¼˜ï¼‰
        max_test_count = int(os.getenv('MAX_PROXY_TEST_COUNT', '5'))
        min_success_rate = float(os.getenv('MIN_SUCCESS_RATE', '0.8'))
        max_latency_ms = int(os.getenv('MAX_LATENCY_MS', '5000'))
        test_count = int(os.getenv('TEST_COUNT', '3'))
        timeout = int(os.getenv('TEST_TIMEOUT', '10'))
        concurrency = int(os.getenv('TEST_CONCURRENCY', '5'))

        # å¯é€‰åœ°å…è®¸è‡ªå®šä¹‰æµ‹è¯• URL åˆ—è¡¨ï¼ˆä»¥é€—å·åˆ†éš”ï¼‰
        custom_test_urls = os.getenv('TEST_URLS')
        test_urls = None
        if custom_test_urls:
            test_urls = [u.strip() for u in custom_test_urls.split(',') if u.strip()]
        
        logger.info("å¼€å§‹æµ‹è¯•ä»£ç†å»¶è¿Ÿï¼Œé€‰æ‹©æœ€ä½³ä»£ç†...")
        best_proxy_info = find_best_proxy_by_latency(
            proxies,
            max_test_count=max_test_count,
            min_success_rate=min_success_rate,
            max_latency_ms=max_latency_ms,
            test_urls=test_urls,
            test_count=test_count,
            timeout=timeout,
            concurrency=concurrency,
        )
        
        if not best_proxy_info:
            logger.error("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä»£ç†ï¼Œä»»åŠ¡ç»“æŸã€‚")
            return
        
        ip_port = best_proxy_info['proxy']
        user = best_proxy_info['user']
        host, port = ip_port.split(':')
        password = "1"
        
        # å¯¹ç”¨æˆ·åè¿›è¡ŒURLç¼–ç ä»¥å¤„ç†ç‰¹æ®Šå­—ç¬¦
        from urllib.parse import quote
        encoded_user = quote(user)
        proxy_url = f"http://{encoded_user}:{password}@{host}:{port}"
        
        logger.info(f"âœ… é€‰æ‹©æœ€ä½³ä»£ç†ï¼Œå‡†å¤‡ä½¿ç”¨ï¼š{proxy_url}")
        logger.info(f"ğŸ“Š ä»£ç†æ€§èƒ½ç»Ÿè®¡ï¼š")
        logger.info(f"   - å¹³å‡å»¶è¿Ÿ: {best_proxy_info['avg_latency_ms']:.1f}ms")
        logger.info(f"   - æˆåŠŸç‡: {best_proxy_info['success_rate']:.1%}")
        logger.info(f"   - æµ‹è¯•æ¬¡æ•°: {best_proxy_info['working_tests']}/{best_proxy_info['test_count']}")
        
        # æ­¥éª¤5: æ›´æ–°æ¸ é“ä»£ç†
        update_channel_proxy(
            base_url=required_env_vars['BASE_URL'],
            admin_id=required_env_vars['ADMIN_ID'],
            admin_token=required_env_vars['ADMIN_TOKEN'],
            channel_ids=channel_ids,
            proxy_url=proxy_url
        )
        
        # ä¿å­˜æ–°çš„MD5å€¼åˆ°ç¼“å­˜
        save_cached_md5(current_md5)
        logger.info("âœ… ä»£ç†æ›´æ–°ä»»åŠ¡å®Œæˆï¼")
        
    except json.JSONDecodeError as e:
        logger.error(f"âŒ JSONè§£æé”™è¯¯ï¼š{str(e)}")
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")


def run_scheduled_task():
    """å®šæ—¶ä»»åŠ¡å…¥å£"""
    logger.info(f"Cronä»»åŠ¡è§¦å‘ï¼å¼€å§‹æ‰§è¡Œï¼š{datetime.now()}")
    main()


if __name__ == "__main__":
    main()
