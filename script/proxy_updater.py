#!/usr/bin/env python3
"""
New-API Channel Proxy Updater
ä¸€ä¸ªå®šæ—¶ä»»åŠ¡ï¼Œç”¨äºå®šæœŸæ›´æ–°æŒ‡å®šæ¸ é“çš„ä»£ç†IP

ä»£ç†æ¥æºï¼šhttps://github.com/TopChina/proxy-list/blob/main/README.md

ä»£ç†æµ‹è¯•è¯´æ˜ï¼š
- ä»£ç†æ¯å°æ—¶æ›´æ–°ä¸€æ¬¡ç”¨æˆ·åï¼Œå¯†ç é»˜è®¤ä¸º1
- å¹¶å‘æµ‹è¯•å€™é€‰ä»£ç†ï¼Œä½¿ç”¨å¸¦è®¤è¯çš„ä»£ç† URL
- ä½¿ç”¨ TTFBï¼ˆé¦–å­—èŠ‚æ—¶é—´ï¼‰è¡¡é‡å»¶è¿Ÿï¼Œé¿å…ä¸‹è½½æ­£æ–‡å½±å“
- å¤š URL Ã— å¤šæ¬¡é‡‡æ ·ï¼ŒæŒ‰æˆåŠŸæ ·æœ¬å»æå€¼åå–å¹³å‡

ç¯å¢ƒå˜é‡ï¼š
- BASE_URL: New-APIå®ä¾‹çš„URL
- ADMIN_ID: ç®¡ç†å‘˜ç”¨æˆ·ID
- ADMIN_TOKEN: ç®¡ç†å‘˜è®¿é—®ä»¤ç‰Œ
- CHANNEL_IDS: éœ€è¦æ›´æ–°ä»£ç†çš„æ¸ é“IDåˆ—è¡¨
- PROXY_REGION: ä»£ç†åœ°åŒº (é»˜è®¤: é¦™æ¸¯)
- MAX_PROXY_TEST_COUNT: æœ€å¤§ä»£ç†æµ‹è¯•æ•°é‡ (é»˜è®¤: 5)
- MIN_SUCCESS_RATE: æœ€ä½æˆåŠŸç‡è¦æ±‚ (é»˜è®¤: 0.8)
- MAX_LATENCY_MS: æœ€å¤§å¯æ¥å—å»¶è¿Ÿ (é»˜è®¤: 5000ms)
- TEST_COUNT: æ¯ä¸ª URL çš„é‡‡æ ·æ¬¡æ•° (é»˜è®¤: 3)
- TEST_TIMEOUT: å•æ¬¡è¯·æ±‚è¶…æ—¶ç§’æ•° (é»˜è®¤: 10)
- TEST_CONCURRENCY: å¹¶å‘å·¥ä½œçº¿ç¨‹æ•° (é»˜è®¤: 5)
- TEST_URLS: è‡ªå®šä¹‰æµ‹è¯• URLï¼ˆé€—å·åˆ†éš”ï¼Œå¯ç•™ç©ºä½¿ç”¨é»˜è®¤204ç«¯ç‚¹ï¼‰
"""

import os
import json
import re
import requests
import logging
import sys
import warnings
import hashlib
import statistics
import math
from typing import List, Tuple, Optional, Dict, Any
from datetime import datetime
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import quote

# ç¦ç”¨ urllib3 çš„ InsecureRequestWarning è­¦å‘Š
warnings.filterwarnings('ignore', message='Unverified HTTPS request')
warnings.filterwarnings('ignore', category=requests.packages.urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('proxy_updater.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ç¼“å­˜æ–‡ä»¶è·¯å¾„
PROXY_CACHE_FILE = 'proxy_cache.json'


class TestConfig:
    """
    æµ‹è¯•é…ç½®ç®¡ç†ç±» - ç®¡ç†ä»£ç†å»¶è¿Ÿæµ‹è¯•çš„å„ç§å‚æ•°é…ç½®
    """
    
    def __init__(self):
        # æµ‹è¯•é…ç½®å‚æ•°
        self.latency_test_samples = int(os.getenv('LATENCY_TEST_SAMPLES', '5'))
        self.latency_test_urls_count = int(os.getenv('LATENCY_TEST_URLS_COUNT', '3'))
        self.latency_outlier_threshold = float(os.getenv('LATENCY_OUTLIER_THRESHOLD', '2.0'))
        self.latency_spike_threshold_ms = int(os.getenv('LATENCY_SPIKE_THRESHOLD_MS', '1000'))
        self.latency_consistency_window = float(os.getenv('LATENCY_CONSISTENCY_WINDOW', '0.3'))
        
        # è¯„åˆ†æƒé‡é…ç½®
        self.weight_performance = float(os.getenv('WEIGHT_PERFORMANCE', '0.4'))
        self.weight_stability = float(os.getenv('WEIGHT_STABILITY', '0.35'))
        self.weight_availability = float(os.getenv('WEIGHT_AVAILABILITY', '0.25'))
        
        # åŸæœ‰é…ç½®ä¿æŒå…¼å®¹
        self.max_test_count = int(os.getenv('MAX_PROXY_TEST_COUNT', '5'))
        self.min_success_rate = float(os.getenv('MIN_SUCCESS_RATE', '0.8'))
        self.max_latency_ms = int(os.getenv('MAX_LATENCY_MS', '5000'))
        self.test_count = int(os.getenv('TEST_COUNT', '3'))
        self.timeout = int(os.getenv('TEST_TIMEOUT', '10'))
        self.concurrency = int(os.getenv('TEST_CONCURRENCY', '5'))
    
    def get_test_urls(self, category: str = 'mixed') -> List[str]:
        """
        æ ¹æ®æµ‹è¯•ç±»åˆ«è·å–æµ‹è¯•URLåˆ—è¡¨
        
        Args:
            category: æµ‹è¯•ç±»åˆ« ('fast', 'standard', 'heavy', 'mixed')
            
        Returns:
            List[str]: æµ‹è¯•URLåˆ—è¡¨
        """
        # å…è®¸ç”¨æˆ·è‡ªå®šä¹‰æµ‹è¯•URL
        custom_test_urls = os.getenv('TEST_URLS')
        if custom_test_urls:
            return [u.strip() for u in custom_test_urls.split(',') if u.strip()]
        
        # å¿«é€Ÿå“åº”ç±»URL (ä¸»è¦ç”¨äºè¿é€šæ€§æµ‹è¯•)
        fast_urls = [
            "http://www.gstatic.com/generate_204",
            "https://www.gstatic.com/generate_204",
            "http://cp.cloudflare.com/generate_204",
            "http://connectivitycheck.gstatic.com/generate_204",
        ]
        
        # æ ‡å‡†APIç±»URL (æ¨¡æ‹ŸçœŸå®APIè¯·æ±‚)
        standard_urls = [
            "https://httpbin.org/status/200",
            "https://jsonplaceholder.typicode.com/posts/1",
            "https://api.github.com/zen",
            "https://httpstat.us/200",
        ]
        
        # é‡è´Ÿè½½ç±»URL (æµ‹è¯•ç¨³å®šæ€§)
        heavy_urls = [
            "https://httpbin.org/delay/1",
            "https://httpbin.org/bytes/1024",
            "https://www.google.com/favicon.ico",
        ]
        
        if category == 'fast':
            return fast_urls[:self.latency_test_urls_count]
        elif category == 'standard':
            return standard_urls[:self.latency_test_urls_count]
        elif category == 'heavy':
            return heavy_urls[:self.latency_test_urls_count]
        else:  # mixed
            # æ··åˆæ¨¡å¼ï¼šå¿«é€Ÿå“åº”ä¸ºä¸»ï¼Œé€‚å½“åŠ å…¥æ ‡å‡†APIæµ‹è¯•
            mixed_urls = fast_urls[:2] + standard_urls[:1]
            return mixed_urls[:self.latency_test_urls_count]


class ErrorHandler:
    """
    é”™è¯¯å¤„ç†å’Œé™çº§ç­–ç•¥ç®¡ç†ç±»
    """
    
    @staticmethod
    def is_recoverable_error(error_type: str) -> bool:
        """
        åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯æ¢å¤
        
        Args:
            error_type: é”™è¯¯ç±»å‹
            
        Returns:
            bool: æ˜¯å¦å¯æ¢å¤
        """
        recoverable_errors = {
            'Timeout', 'Connection Error', 'HTTP 502', 'HTTP 503', 'HTTP 504'
        }
        return error_type in recoverable_errors
    
    @staticmethod
    def should_use_fallback_strategy(successful_samples: int, total_samples: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨é™çº§ç­–ç•¥
        
        Args:
            successful_samples: æˆåŠŸæ ·æœ¬æ•°
            total_samples: æ€»æ ·æœ¬æ•°
            
        Returns:
            bool: æ˜¯å¦ä½¿ç”¨é™çº§ç­–ç•¥
        """
        if total_samples == 0:
            return True
        
        success_rate = successful_samples / total_samples
        min_samples = 3
        min_success_rate = 0.3
        
        return successful_samples < min_samples or success_rate < min_success_rate
    
    @staticmethod
    def get_fallback_urls() -> List[str]:
        """
        è·å–é™çº§æµ‹è¯•URLï¼ˆæœ€å¯é çš„URLï¼‰
        
        Returns:
            List[str]: é™çº§æµ‹è¯•URLåˆ—è¡¨
        """
        return [
            "http://www.gstatic.com/generate_204",
            "https://www.gstatic.com/generate_204"
        ]


class LatencyStatistics:
    """
    å»¶è¿Ÿç»Ÿè®¡è®¡ç®—å™¨ - æä¾›å…¨é¢çš„å»¶è¿Ÿåˆ†ææŒ‡æ ‡
    
    æ”¯æŒè®¡ç®—åŸºç¡€ç»Ÿè®¡ã€å˜å¼‚æ€§æŒ‡æ ‡ã€ç¨³å¥æ€§æŒ‡æ ‡å’ŒAPIåœºæ™¯ç‰¹å®šæŒ‡æ ‡
    """
    
    @staticmethod
    def calculate_basic_stats(latencies: List[float]) -> Dict[str, Optional[float]]:
        """
        è®¡ç®—åŸºç¡€ç»Ÿè®¡æŒ‡æ ‡
        
        Args:
            latencies: å»¶è¿Ÿæ•°æ®åˆ—è¡¨ (æ¯«ç§’)
            
        Returns:
            Dict: åŒ…å«åŸºç¡€ç»Ÿè®¡æŒ‡æ ‡çš„å­—å…¸
        """
        if not latencies:
            return {
                'mean_latency_ms': None,
                'median_latency_ms': None,
                'min_latency_ms': None,
                'max_latency_ms': None,
                'p25_latency_ms': None,
                'p75_latency_ms': None,
                'p95_latency_ms': None,
                'p99_latency_ms': None
            }
        
        sorted_latencies = sorted(latencies)
        
        # åŸºç¡€ç»Ÿè®¡æŒ‡æ ‡
        mean_latency = statistics.mean(latencies)
        median_latency = statistics.median(latencies)
        min_latency = min(latencies)
        max_latency = max(latencies)
        
        # è®¡ç®—åˆ†ä½æ•°
        p25 = LatencyStatistics._percentile(sorted_latencies, 25)
        p75 = LatencyStatistics._percentile(sorted_latencies, 75)
        p95 = LatencyStatistics._percentile(sorted_latencies, 95)
        p99 = LatencyStatistics._percentile(sorted_latencies, 99)
        
        return {
            'mean_latency_ms': mean_latency,
            'median_latency_ms': median_latency,
            'min_latency_ms': min_latency,
            'max_latency_ms': max_latency,
            'p25_latency_ms': p25,
            'p75_latency_ms': p75,
            'p95_latency_ms': p95,
            'p99_latency_ms': p99
        }
    
    @staticmethod
    def calculate_variability_stats(latencies: List[float]) -> Dict[str, Optional[float]]:
        """
        è®¡ç®—å˜å¼‚æ€§æŒ‡æ ‡
        
        Args:
            latencies: å»¶è¿Ÿæ•°æ®åˆ—è¡¨ (æ¯«ç§’)
            
        Returns:
            Dict: åŒ…å«å˜å¼‚æ€§æŒ‡æ ‡çš„å­—å…¸
        """
        if not latencies or len(latencies) < 2:
            return {
                'std_dev_ms': None,
                'coefficient_variation': None,
                'iqr_ms': None,
                'robust_std_dev_ms': None,
                'mad_ms': None
            }
        
        # åŸºç¡€å˜å¼‚æ€§æŒ‡æ ‡
        mean_latency = statistics.mean(latencies)
        std_dev = statistics.stdev(latencies)
        coefficient_variation = std_dev / mean_latency if mean_latency > 0 else None
        
        # ç¨³å¥å˜å¼‚æ€§æŒ‡æ ‡
        sorted_latencies = sorted(latencies)
        p25 = LatencyStatistics._percentile(sorted_latencies, 25)
        p75 = LatencyStatistics._percentile(sorted_latencies, 75)
        iqr = p75 - p25 if p25 is not None and p75 is not None else None
        
        # ä¸­ä½æ•°ç»å¯¹åå·® (MAD)
        median_latency = statistics.median(latencies)
        mad = statistics.median([abs(x - median_latency) for x in latencies])
        
        # ç¨³å¥æ ‡å‡†å·® (åŸºäºMAD)
        robust_std_dev = mad * 1.4826 if mad is not None else None
        
        return {
            'std_dev_ms': std_dev,
            'coefficient_variation': coefficient_variation,
            'iqr_ms': iqr,
            'robust_std_dev_ms': robust_std_dev,
            'mad_ms': mad
        }
    
    @staticmethod
    def calculate_robustness_stats(latencies: List[float]) -> Dict[str, Optional[float]]:
        """
        è®¡ç®—ç¨³å¥æ€§æŒ‡æ ‡
        
        Args:
            latencies: å»¶è¿Ÿæ•°æ®åˆ—è¡¨ (æ¯«ç§’)
            
        Returns:
            Dict: åŒ…å«ç¨³å¥æ€§æŒ‡æ ‡çš„å­—å…¸
        """
        if not latencies:
            return {
                'consistency_score': None,
                'stability_index': None,
                'outlier_ratio': None,
                'trimmed_mean_ms': None
            }
        
        # ä¸€è‡´æ€§è¯„åˆ† (åŸºäºå˜å¼‚ç³»æ•°çš„å€’æ•°)
        mean_latency = statistics.mean(latencies)
        if len(latencies) >= 2 and mean_latency > 0:
            std_dev = statistics.stdev(latencies)
            cv = std_dev / mean_latency
            consistency_score = 1 / (1 + cv) if cv >= 0 else 0
        else:
            consistency_score = 1.0
        
        # ç¨³å®šæ€§æŒ‡æ•° (åŸºäºIQRçš„å½’ä¸€åŒ–)
        sorted_latencies = sorted(latencies)
        p25 = LatencyStatistics._percentile(sorted_latencies, 25)
        p75 = LatencyStatistics._percentile(sorted_latencies, 75)
        if p25 is not None and p75 is not None and mean_latency > 0:
            iqr = p75 - p25
            stability_index = 1 / (1 + iqr / mean_latency)
        else:
            stability_index = 1.0
        
        # å¼‚å¸¸å€¼æ¯”ä¾‹ (ä½¿ç”¨1.5å€IQRè§„åˆ™)
        outlier_count = 0
        if p25 is not None and p75 is not None:
            iqr = p75 - p25
            lower_bound = p25 - 1.5 * iqr
            upper_bound = p75 + 1.5 * iqr
            outlier_count = sum(1 for x in latencies if x < lower_bound or x > upper_bound)
        outlier_ratio = outlier_count / len(latencies) if latencies else 0
        
        # å»æå€¼å¹³å‡æ•° (10%æˆªå°¾å¹³å‡)
        trimmed_mean = LatencyStatistics._trimmed_mean(latencies, 0.1)
        
        return {
            'consistency_score': consistency_score,
            'stability_index': stability_index,
            'outlier_ratio': outlier_ratio,
            'trimmed_mean_ms': trimmed_mean
        }
    
    @staticmethod
    def calculate_api_performance_stats(
        latencies: List[float], 
        success_rate: float,
        spike_threshold_ms: float = 1000.0,
        timeout_threshold_ms: float = 10000.0
    ) -> Dict[str, Optional[float]]:
        """
        è®¡ç®—APIåœºæ™¯ç‰¹å®šæŒ‡æ ‡
        
        Args:
            latencies: å»¶è¿Ÿæ•°æ®åˆ—è¡¨ (æ¯«ç§’)
            success_rate: æˆåŠŸç‡ (0-1)
            spike_threshold_ms: çªå‘å»¶è¿Ÿé˜ˆå€¼ (æ¯«ç§’)
            timeout_threshold_ms: è¶…æ—¶é˜ˆå€¼ (æ¯«ç§’)
            
        Returns:
            Dict: åŒ…å«APIæ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        if not latencies:
            return {
                'spike_rate': None,
                'timeout_risk_score': None,
                'availability_stability': success_rate,
                'qos_score': None,
                'sustained_performance_score': None
            }
        
        # çªå‘å»¶è¿Ÿç‡
        spike_count = sum(1 for x in latencies if x > spike_threshold_ms)
        spike_rate = spike_count / len(latencies)
        
        # è¶…æ—¶é£é™©è¯„åˆ† (åŸºäºP95å»¶è¿Ÿ)
        sorted_latencies = sorted(latencies)
        p95 = LatencyStatistics._percentile(sorted_latencies, 95)
        if p95 is not None:
            timeout_risk_score = min(1.0, p95 / timeout_threshold_ms)
        else:
            timeout_risk_score = 0.0
        
        # å¯ç”¨æ€§ç¨³å®šæ€§ (å°±æ˜¯æˆåŠŸç‡)
        availability_stability = success_rate
        
        # QoSç»¼åˆè¯„åˆ† (è€ƒè™‘å»¶è¿Ÿã€ç¨³å®šæ€§å’Œå¯ç”¨æ€§)
        mean_latency = statistics.mean(latencies)
        # å»¶è¿Ÿæ€§èƒ½è¯„åˆ† (è¶Šä½è¶Šå¥½ï¼Œå½’ä¸€åŒ–åˆ°0-1)
        latency_score = max(0, 1 - mean_latency / 5000)  # 5ç§’ä½œä¸ºåŸºå‡†
        
        # ç¨³å®šæ€§è¯„åˆ† (åŸºäºå˜å¼‚ç³»æ•°)
        if len(latencies) >= 2:
            std_dev = statistics.stdev(latencies)
            cv = std_dev / mean_latency if mean_latency > 0 else 0
            stability_score = 1 / (1 + cv)
        else:
            stability_score = 1.0
        
        # QoSç»¼åˆè¯„åˆ† (æƒé‡ï¼šå»¶è¿Ÿ40%ï¼Œç¨³å®šæ€§35%ï¼Œå¯ç”¨æ€§25%)
        qos_score = (
            latency_score * 0.4 + 
            stability_score * 0.35 + 
            availability_stability * 0.25
        )
        
        # æŒç»­æ€§èƒ½è¯„åˆ† (ç»“åˆå¤šä¸ªç»´åº¦)
        sustained_performance_score = (
            (1 - spike_rate) * 0.3 +
            (1 - timeout_risk_score) * 0.3 +
            stability_score * 0.4
        )
        
        return {
            'spike_rate': spike_rate,
            'timeout_risk_score': timeout_risk_score,
            'availability_stability': availability_stability,
            'qos_score': qos_score,
            'sustained_performance_score': sustained_performance_score
        }
    
    @staticmethod
    def _percentile(sorted_data: List[float], percentile: float) -> Optional[float]:
        """
        è®¡ç®—ç™¾åˆ†ä½æ•°
        
        Args:
            sorted_data: å·²æ’åºçš„æ•°æ®
            percentile: ç™¾åˆ†ä½æ•° (0-100)
            
        Returns:
            Optional[float]: ç™¾åˆ†ä½æ•°å€¼
        """
        if not sorted_data:
            return None
        
        if percentile <= 0:
            return sorted_data[0]
        if percentile >= 100:
            return sorted_data[-1]
        
        index = (len(sorted_data) - 1) * percentile / 100
        lower_index = int(math.floor(index))
        upper_index = int(math.ceil(index))
        
        if lower_index == upper_index:
            return sorted_data[lower_index]
        
        # çº¿æ€§æ’å€¼
        weight = index - lower_index
        return sorted_data[lower_index] * (1 - weight) + sorted_data[upper_index] * weight
    
    @staticmethod
    def _trimmed_mean(data: List[float], trim_ratio: float) -> Optional[float]:
        """
        è®¡ç®—æˆªå°¾å¹³å‡æ•°
        
        Args:
            data: æ•°æ®åˆ—è¡¨
            trim_ratio: æˆªå°¾æ¯”ä¾‹ (0-0.5)
            
        Returns:
            Optional[float]: æˆªå°¾å¹³å‡æ•°
        """
        if not data:
            return None
        
        if trim_ratio <= 0:
            return statistics.mean(data)
        
        sorted_data = sorted(data)
        n = len(sorted_data)
        trim_count = int(n * trim_ratio)
        
        if trim_count * 2 >= n:
            return statistics.median(data)
        
        trimmed_data = sorted_data[trim_count:n-trim_count]
        return statistics.mean(trimmed_data) if trimmed_data else None


class ProxyEvaluator:
    """
    ä»£ç†è¯„ä¼°å™¨ - æä¾›ç»¼åˆä»£ç†è´¨é‡è¯„ä¼°å’Œæ’åºåŠŸèƒ½
    
    æ”¯æŒå¤šç»´åº¦è¯„åˆ†å’Œåœºæ™¯é€‚åº”æ€§è°ƒæ•´
    """
    
    def __init__(self, weights: Optional[Dict[str, float]] = None):
        """
        åˆå§‹åŒ–ä»£ç†è¯„ä¼°å™¨
        
        Args:
            weights: è¯„åˆ†æƒé‡é…ç½®
        """
        # é»˜è®¤æƒé‡é…ç½®
        default_weights = {
            'performance': 0.4,     # æ€§èƒ½æƒé‡ 40%
            'stability': 0.35,      # ç¨³å®šæ€§æƒé‡ 35%
            'availability': 0.25    # å¯ç”¨æ€§æƒé‡ 25%
        }
        
        self.weights = weights if weights else default_weights
        
        # å­æŒ‡æ ‡æƒé‡é…ç½®
        self.performance_sub_weights = {
            'mean_latency': 0.5,
            'p95_latency': 0.3,
            'median_latency': 0.2
        }
        
        self.stability_sub_weights = {
            'coefficient_variation': 0.4,
            'consistency_score': 0.35,
            'outlier_ratio': 0.25
        }
        
        self.availability_sub_weights = {
            'success_rate': 0.6,
            'connection_stability': 0.25,
            'timeout_risk': 0.15
        }
    
    def calculate_performance_score(self, basic_stats: Dict, variability_stats: Dict) -> float:
        """
        è®¡ç®—æ€§èƒ½è¯„åˆ†
        
        Args:
            basic_stats: åŸºç¡€ç»Ÿè®¡æŒ‡æ ‡
            variability_stats: å˜å¼‚æ€§æŒ‡æ ‡
            
        Returns:
            float: æ€§èƒ½è¯„åˆ† (0-1)
        """
        mean_latency = basic_stats.get('mean_latency_ms', float('inf'))
        p95_latency = basic_stats.get('p95_latency_ms', float('inf'))
        median_latency = basic_stats.get('median_latency_ms', float('inf'))
        
        # å°†å»¶è¿Ÿè½¬æ¢ä¸ºè¯„åˆ† (0-1ï¼Œè¶Šä½è¶Šå¥½)
        mean_score = max(0, 1 - mean_latency / 5000) if mean_latency != float('inf') else 0
        p95_score = max(0, 1 - p95_latency / 8000) if p95_latency != float('inf') else 0
        median_score = max(0, 1 - median_latency / 4000) if median_latency != float('inf') else 0
        
        # åŠ æƒå¹³å‡
        performance_score = (
            mean_score * self.performance_sub_weights['mean_latency'] +
            p95_score * self.performance_sub_weights['p95_latency'] +
            median_score * self.performance_sub_weights['median_latency']
        )
        
        return min(1.0, max(0.0, performance_score))
    
    def calculate_stability_score(self, variability_stats: Dict, robustness_stats: Dict) -> float:
        """
        è®¡ç®—ç¨³å®šæ€§è¯„åˆ†
        
        Args:
            variability_stats: å˜å¼‚æ€§æŒ‡æ ‡
            robustness_stats: ç¨³å¥æ€§æŒ‡æ ‡
            
        Returns:
            float: ç¨³å®šæ€§è¯„åˆ† (0-1)
        """
        cv = variability_stats.get('coefficient_variation', float('inf'))
        consistency_score = robustness_stats.get('consistency_score', 0)
        outlier_ratio = robustness_stats.get('outlier_ratio', 1)
        
        # å˜å¼‚ç³»æ•°è¯„åˆ† (è¶Šä½è¶Šå¥½)
        cv_score = 1 / (1 + cv) if cv != float('inf') else 0
        
        # ä¸€è‡´æ€§è¯„åˆ† (å·²ç»æ˜¯0-1èŒƒå›´)
        consistency = consistency_score if consistency_score is not None else 0
        
        # å¼‚å¸¸å€¼è¯„åˆ† (è¶Šä½è¶Šå¥½)
        outlier_score = 1 - outlier_ratio
        
        # åŠ æƒå¹³å‡
        stability_score = (
            cv_score * self.stability_sub_weights['coefficient_variation'] +
            consistency * self.stability_sub_weights['consistency_score'] +
            outlier_score * self.stability_sub_weights['outlier_ratio']
        )
        
        return min(1.0, max(0.0, stability_score))
    
    def calculate_availability_score(
        self, 
        success_rate: float, 
        api_performance_stats: Dict
    ) -> float:
        """
        è®¡ç®—å¯ç”¨æ€§è¯„åˆ†
        
        Args:
            success_rate: æˆåŠŸç‡ (0-1)
            api_performance_stats: APIæ€§èƒ½æŒ‡æ ‡
            
        Returns:
            float: å¯ç”¨æ€§è¯„åˆ† (0-1)
        """
        # æˆåŠŸç‡è¯„åˆ†
        success_score = success_rate
        
        # è¿æ¥ç¨³å®šæ€§ (åŸºäºå¯ç”¨æ€§ç¨³å®šæ€§)
        connection_stability = api_performance_stats.get('availability_stability', success_rate)
        
        # è¶…æ—¶é£é™©è¯„åˆ† (è¶Šä½è¶Šå¥½)
        timeout_risk = api_performance_stats.get('timeout_risk_score', 0)
        timeout_score = 1 - timeout_risk
        
        # åŠ æƒå¹³å‡
        availability_score = (
            success_score * self.availability_sub_weights['success_rate'] +
            connection_stability * self.availability_sub_weights['connection_stability'] +
            timeout_score * self.availability_sub_weights['timeout_risk']
        )
        
        return min(1.0, max(0.0, availability_score))
    
    def calculate_composite_score(self, stats: Dict) -> float:
        """
        è®¡ç®—ç»¼åˆè¯„åˆ†
        
        Args:
            stats: å®Œæ•´çš„ç»Ÿè®¡æ•°æ®
            
        Returns:
            float: ç»¼åˆè¯„åˆ† (0-1)
        """
        basic_stats = stats.get('basic_stats', {})
        variability_stats = stats.get('variability_stats', {})
        robustness_stats = stats.get('robustness_stats', {})
        api_performance_stats = stats.get('api_performance', {})
        success_rate = stats.get('success_rate', 0)
        
        # è®¡ç®—å„ç»´åº¦è¯„åˆ†
        performance_score = self.calculate_performance_score(basic_stats, variability_stats)
        stability_score = self.calculate_stability_score(variability_stats, robustness_stats)
        availability_score = self.calculate_availability_score(success_rate, api_performance_stats)
        
        # ç»¼åˆè¯„åˆ†
        composite_score = (
            performance_score * self.weights['performance'] +
            stability_score * self.weights['stability'] +
            availability_score * self.weights['availability']
        )
        
        return min(1.0, max(0.0, composite_score))
    
    def calculate_qos_score(self, stats: Dict) -> float:
        """
        è®¡ç®—QoSè¯„åˆ† (ä» API æ€§èƒ½æŒ‡æ ‡ä¸­ç›´æ¥è·å–æˆ–è®¡ç®—)
        
        Args:
            stats: å®Œæ•´çš„ç»Ÿè®¡æ•°æ®
            
        Returns:
            float: QoSè¯„åˆ† (0-1)
        """
        api_performance = stats.get('api_performance', {})
        qos_score = api_performance.get('qos_score')
        
        if qos_score is not None:
            return qos_score
        
        # å¦‚æœæ²¡æœ‰ç›´æ¥çš„QoSè¯„åˆ†ï¼Œä½¿ç”¨ç»¼åˆè¯„åˆ†
        return self.calculate_composite_score(stats)
    
    def rank_proxies(self, proxy_results: List[Dict]) -> List[Dict]:
        """
        å¯¹ä»£ç†åˆ—è¡¨è¿›è¡Œæ’åº
        
        Args:
            proxy_results: ä»£ç†æµ‹è¯•ç»“æœåˆ—è¡¨
            
        Returns:
            List[Dict]: æ’åºåçš„ä»£ç†åˆ—è¡¨
        """
        # ä¸ºæ¯ä¸ªä»£ç†è®¡ç®—ç»¼åˆè¯„åˆ†
        for proxy in proxy_results:
            if proxy.get('is_working') and proxy.get('enhanced_stats'):
                proxy['composite_score'] = self.calculate_composite_score(proxy['enhanced_stats'])
                proxy['qos_score'] = self.calculate_qos_score(proxy['enhanced_stats'])
            else:
                proxy['composite_score'] = 0.0
                proxy['qos_score'] = 0.0
        
        # æŒ‰ç»¼åˆè¯„åˆ†é™åºæ’åˆ—
        ranked_proxies = sorted(
            proxy_results,
            key=lambda x: (
                x.get('is_working', False),  # é¦–å…ˆæŒ‰æ˜¯å¦å¯ç”¨
                x.get('composite_score', 0)  # å…¶æ¬¡æŒ‰ç»¼åˆè¯„åˆ†
            ),
            reverse=True
        )
        
        return ranked_proxies
    
    def select_best_proxy(
        self, 
        proxy_results: List[Dict],
        min_composite_score: float = 0.6,
        min_qos_score: float = 0.5
    ) -> Optional[Dict]:
        """
        é€‰æ‹©æœ€ä½³ä»£ç†
        
        Args:
            proxy_results: ä»£ç†æµ‹è¯•ç»“æœåˆ—è¡¨
            min_composite_score: æœ€ä½ç»¼åˆè¯„åˆ†è¦æ±‚
            min_qos_score: æœ€ä½QoSè¯„åˆ†è¦æ±‚
            
        Returns:
            Optional[Dict]: æœ€ä½³ä»£ç†ï¼Œæ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„è¿”å›None
        """
        ranked_proxies = self.rank_proxies(proxy_results)
        
        for proxy in ranked_proxies:
            if (
                proxy.get('is_working') and
                proxy.get('composite_score', 0) >= min_composite_score and
                proxy.get('qos_score', 0) >= min_qos_score
            ):
                return proxy
        
        # å¦‚æœæ²¡æœ‰ä»£ç†æ»¡è¶³ä¸¥æ ¼æ¡ä»¶ï¼Œè¿”å›è¯„åˆ†æœ€é«˜çš„å¯ç”¨ä»£ç†
        for proxy in ranked_proxies:
            if proxy.get('is_working'):
                logger.warning(
                    f"æ²¡æœ‰ä»£ç†è¾¾åˆ°ä¸¥æ ¼æ ‡å‡†ï¼ˆç»¼åˆè¯„åˆ†>={min_composite_score}, QoS>={min_qos_score}ï¼‰ï¼Œ"
                    f"é€‰æ‹©è¯„åˆ†æœ€é«˜çš„å¯ç”¨ä»£ç†: {proxy.get('proxy')}"
                )
                return proxy
        
        return None


class EnhancedLatencyTester:
    """
    å¢å¼ºçš„å»¶è¿Ÿæµ‹è¯•å™¨ - é›†æˆç»Ÿè®¡åˆ†æå’Œè¯„ä¼°åŠŸèƒ½
    """

    def __init__(self, test_config: Optional[TestConfig] = None):
        """
        åˆå§‹åŒ–å¢å¼ºçš„å»¶è¿Ÿæµ‹è¯•å™¨
        
        Args:
            test_config: æµ‹è¯•é…ç½®å¯¹è±¡
        """
        self.config = test_config if test_config else TestConfig()
        self.evaluator = ProxyEvaluator({
            'performance': self.config.weight_performance,
            'stability': self.config.weight_stability,
            'availability': self.config.weight_availability
        })
        self.error_handler = ErrorHandler()
    
    def measure_proxy_latency_enhanced(
        self,
        proxy_ip: str,
        proxy_port: str,
        proxy_user: str,
        proxy_password: str = "1",
        test_urls: Optional[List[str]] = None,
        test_count: Optional[int] = None,
        timeout: Optional[int] = None,
        use_fallback: bool = True
    ) -> Dict:
        """
        å¢å¼ºçš„ä»£ç†å»¶è¿Ÿæµ‹è¯•
        
        Args:
            proxy_ip: ä»£ç†IPåœ°å€
            proxy_port: ä»£ç†ç«¯å£
            proxy_user: ä»£ç†ç”¨æˆ·å
            proxy_password: ä»£ç†å¯†ç 
            test_urls: æµ‹è¯•URLåˆ—è¡¨
            test_count: æµ‹è¯•æ¬¡æ•°
            timeout: è¶…æ—¶æ—¶é—´
            use_fallback: æ˜¯å¦ä½¿ç”¨é™çº§ç­–ç•¥
            
        Returns:
            Dict: å¢å¼ºçš„æµ‹è¯•ç»“æœ
        """
        # ä½¿ç”¨é…ç½®é»˜è®¤å€¼
        if test_urls is None:
            test_urls = self.config.get_test_urls('mixed')
        if test_count is None:
            test_count = self.config.latency_test_samples
        if timeout is None:
            timeout = self.config.timeout
        
        all_results = []
        working_tests = 0
        test_details = []
        
        logger.debug(f"å¼€å§‹å¢å¼ºå»¶è¿Ÿæµ‹è¯•: {proxy_ip}:{proxy_port}")
        
        # æ‰§è¡Œæµ‹è¯•
        for test_url in test_urls:
            url_results = []
            for i in range(test_count):
                result = test_proxy_connectivity(
                    proxy_ip, proxy_port, proxy_user, proxy_password, test_url, timeout
                )
                url_results.append(result)
                
                if result['is_working'] and result['latency_ms'] is not None:
                    working_tests += 1
                    all_results.append(result['latency_ms'])
                
                # çŸ­æš‚å»¶è¿Ÿ
                if i < test_count - 1:
                    time.sleep(0.2)
            
            test_details.append({
                'url': test_url,
                'results': url_results
            })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é™çº§ç­–ç•¥
        total_tests = len(test_urls) * test_count
        if use_fallback and self.error_handler.should_use_fallback_strategy(working_tests, total_tests):
            logger.warning(f"ä»£ç† {proxy_ip}:{proxy_port} æ ·æœ¬ä¸è¶³ï¼Œå¯ç”¨é™çº§ç­–ç•¥")
            return self._fallback_test(proxy_ip, proxy_port, proxy_user, proxy_password, timeout)
        
        # æ²¡æœ‰æœ‰æ•ˆæ•°æ®
        if not working_tests:
            return self._create_failed_result(proxy_ip, proxy_port, proxy_user, test_details, total_tests)
        
        # è®¡ç®—å¢å¼ºç»Ÿè®¡æŒ‡æ ‡
        enhanced_stats = self._calculate_enhanced_stats(all_results, working_tests, total_tests)
        
        # åˆ›å»ºç»“æœå¯¹è±¡
        result = {
            'proxy': f"{proxy_ip}:{proxy_port}",
            'user': proxy_user,
            'is_working': True,
            
            # åŸæœ‰æŒ‡æ ‡ä¿æŒå…¼å®¹
            'avg_latency_ms': enhanced_stats['basic_stats']['mean_latency_ms'],
            'min_latency_ms': enhanced_stats['basic_stats']['min_latency_ms'],
            'max_latency_ms': enhanced_stats['basic_stats']['max_latency_ms'],
            'success_rate': enhanced_stats['success_rate'],
            'test_count': total_tests,
            'working_tests': working_tests,
            'test_details': test_details,
            
            # æ–°å¢å¢å¼ºç»Ÿè®¡æ•°æ®
            'enhanced_stats': enhanced_stats
        }
        
        # è®¡ç®—è¯„åˆ†
        result['composite_score'] = self.evaluator.calculate_composite_score(enhanced_stats)
        result['qos_score'] = self.evaluator.calculate_qos_score(enhanced_stats)
        
        logger.debug(
            f"âœ… ä»£ç† {proxy_ip}:{proxy_port} å¢å¼ºæµ‹è¯•å®Œæˆ: "
            f"å¹³å‡={enhanced_stats['basic_stats']['mean_latency_ms']:.1f}ms, "
            f"æˆåŠŸç‡={enhanced_stats['success_rate']:.1%}, "
            f"ç»¼åˆè¯„åˆ†={result['composite_score']:.3f}"
        )
        
        return result
    
    def _calculate_enhanced_stats(self, latencies: List[float], working_tests: int, total_tests: int) -> Dict:
        """
        è®¡ç®—å¢å¼ºç»Ÿè®¡æŒ‡æ ‡
        
        Args:
            latencies: å»¶è¿Ÿæ•°æ®åˆ—è¡¨
            working_tests: æˆåŠŸæµ‹è¯•æ¬¡æ•°
            total_tests: æ€»æµ‹è¯•æ¬¡æ•°
            
        Returns:
            Dict: å¢å¼ºç»Ÿè®¡æŒ‡æ ‡
        """
        success_rate = working_tests / total_tests if total_tests > 0 else 0
        
        # è®¡ç®—å„ç±»ç»Ÿè®¡æŒ‡æ ‡
        basic_stats = LatencyStatistics.calculate_basic_stats(latencies)
        variability_stats = LatencyStatistics.calculate_variability_stats(latencies)
        robustness_stats = LatencyStatistics.calculate_robustness_stats(latencies)
        api_performance_stats = LatencyStatistics.calculate_api_performance_stats(
            latencies, 
            success_rate,
            self.config.latency_spike_threshold_ms,
            self.config.timeout * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        )
        
        return {
            'basic_stats': basic_stats,
            'variability_stats': variability_stats,
            'robustness_stats': robustness_stats,
            'api_performance': api_performance_stats,
            'success_rate': success_rate,
            'test_metadata': {
                'total_requests': total_tests,
                'successful_requests': working_tests,
                'sampling_strategy': 'enhanced_multi_url_weighted'
            }
        }
    
    def _fallback_test(
        self, 
        proxy_ip: str, 
        proxy_port: str, 
        proxy_user: str, 
        proxy_password: str, 
        timeout: int
    ) -> Dict:
        """
        é™çº§æµ‹è¯•ç­–ç•¥
        
        Args:
            proxy_ip: ä»£ç†IP
            proxy_port: ä»£ç†ç«¯å£
            proxy_user: ä»£ç†ç”¨æˆ·å
            proxy_password: ä»£ç†å¯†ç 
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            Dict: é™çº§æµ‹è¯•ç»“æœ
        """
        fallback_urls = self.error_handler.get_fallback_urls()
        fallback_results = []
        working_tests = 0
        
        logger.debug(f"ä½¿ç”¨é™çº§ç­–ç•¥æµ‹è¯•ä»£ç†: {proxy_ip}:{proxy_port}")
        
        for url in fallback_urls:
            for _ in range(3):  # æ“ä½œ3æ¬¡
                result = test_proxy_connectivity(
                    proxy_ip, proxy_port, proxy_user, proxy_password, url, timeout
                )
                if result['is_working'] and result['latency_ms'] is not None:
                    working_tests += 1
                    fallback_results.append(result['latency_ms'])
                time.sleep(0.1)
        
        total_fallback_tests = len(fallback_urls) * 3
        
        if not working_tests:
            return self._create_failed_result(proxy_ip, proxy_port, proxy_user, [], total_fallback_tests)
        
        # ä½¿ç”¨ç®€åŒ–çš„ç»Ÿè®¡æŒ‡æ ‡
        enhanced_stats = self._calculate_enhanced_stats(fallback_results, working_tests, total_fallback_tests)
        
        result = {
            'proxy': f"{proxy_ip}:{proxy_port}",
            'user': proxy_user,
            'is_working': True,
            'avg_latency_ms': enhanced_stats['basic_stats']['mean_latency_ms'],
            'min_latency_ms': enhanced_stats['basic_stats']['min_latency_ms'],
            'max_latency_ms': enhanced_stats['basic_stats']['max_latency_ms'],
            'success_rate': enhanced_stats['success_rate'],
            'test_count': total_fallback_tests,
            'working_tests': working_tests,
            'test_details': [],
            'enhanced_stats': enhanced_stats,
            'is_fallback': True  # æ ‡è®°ä¸ºé™çº§ç»“æœ
        }
        
        result['composite_score'] = self.evaluator.calculate_composite_score(enhanced_stats)
        result['qos_score'] = self.evaluator.calculate_qos_score(enhanced_stats)
        
        logger.warning(
            f"âš ï¸ ä»£ç† {proxy_ip}:{proxy_port} é™çº§æµ‹è¯•å®Œæˆ: "
            f"å¹³å‡={enhanced_stats['basic_stats']['mean_latency_ms']:.1f}ms, "
            f"æˆåŠŸç‡={enhanced_stats['success_rate']:.1%}"
        )
        
        return result
    
    def _create_failed_result(
        self, 
        proxy_ip: str, 
        proxy_port: str, 
        proxy_user: str, 
        test_details: List[Dict], 
        total_tests: int
    ) -> Dict:
        """
        åˆ›å»ºå¤±è´¥ç»“æœ
        
        Args:
            proxy_ip: ä»£ç†IP
            proxy_port: ä»£ç†ç«¯å£
            proxy_user: ä»£ç†ç”¨æˆ·å
            test_details: æµ‹è¯•è¯¦æƒ…
            total_tests: æ€»æµ‹è¯•æ¬¡æ•°
            
        Returns:
            Dict: å¤±è´¥ç»“æœ
        """
        logger.debug(f"âŒ ä»£ç† {proxy_ip}:{proxy_port} æ‰€æœ‰æµ‹è¯•å‡å¤±è´¥")
        
        return {
            'proxy': f"{proxy_ip}:{proxy_port}",
            'user': proxy_user,
            'is_working': False,
            'avg_latency_ms': None,
            'min_latency_ms': None,
            'max_latency_ms': None,
            'success_rate': 0.0,
            'test_count': total_tests,
            'working_tests': 0,
            'test_details': test_details,
            'enhanced_stats': None,
            'composite_score': 0.0,
            'qos_score': 0.0
        }
    
    def batch_test_proxies(self, proxies: List[Tuple[str, str]]) -> List[Dict]:
        """
        æ‰¹é‡æµ‹è¯•ä»£ç†
        
        Args:
            proxies: ä»£ç†åˆ—è¡¨ [(ip_port, user), ...]
            
        Returns:
            List[Dict]: æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹æ‰¹é‡æµ‹è¯• {len(proxies)} ä¸ªä»£ç†")
        
        results = []
        with ThreadPoolExecutor(max_workers=self.config.concurrency) as executor:
            future_map = {}
            
            for ip_port, user in proxies:
                try:
                    proxy_ip, proxy_port = ip_port.split(":")
                    future = executor.submit(
                        self.measure_proxy_latency_enhanced,
                        proxy_ip, proxy_port, user
                    )
                    future_map[future] = (ip_port, user)
                except ValueError:
                    # æ— æ•ˆçš„ ip:port æ ¼å¼
                    results.append({
                        'proxy': ip_port,
                        'user': user,
                        'is_working': False,
                        'error': 'Invalid ip:port format',
                        'composite_score': 0.0,
                        'qos_score': 0.0
                    })
            
            for future in as_completed(future_map):
                ip_port, user = future_map[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    logger.warning(f"ä»£ç† {ip_port} æµ‹è¯•å¼‚å¸¸: {e}")
                    results.append({
                        'proxy': ip_port,
                        'user': user,
                        'is_working': False,
                        'error': str(e),
                        'composite_score': 0.0,
                        'qos_score': 0.0
                    })
        
        return results


def find_best_proxy_by_latency_enhanced(
    proxies: List[Tuple[str, str]],
    max_test_count: int = 5,
    min_success_rate: float = 0.8,
    max_latency_ms: int = 5000,
    min_composite_score: float = 0.6,
    min_qos_score: float = 0.5,
    test_config: Optional[TestConfig] = None
) -> Optional[Dict]:
    """
    å¢å¼ºç‰ˆæœ¬çš„ä»£ç†å»¶è¿Ÿæµ‹è¯•å’Œè¯„ä¼°
    
    Args:
        proxies: ä»£ç†åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(ip_port, user)å…ƒç»„
        max_test_count: æœ€å¤§æµ‹è¯•æ•°é‡
        min_success_rate: æœ€ä½æˆåŠŸç‡è¦æ±‚
        max_latency_ms: æœ€å¤§å¯æ¥å—å»¶è¿Ÿ
        min_composite_score: æœ€ä½ç»¼åˆè¯„åˆ†è¦æ±‚
        min_qos_score: æœ€ä½QoSè¯„åˆ†è¦æ±‚
        test_config: æµ‹è¯•é…ç½®
        
    Returns:
        Optional[Dict]: æœ€ä½³ä»£ç†çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
    """
    logger.info(f"å¼€å§‹å¢å¼ºä»£ç†å»¶è¿Ÿæµ‹è¯•ï¼Œå…± {len(proxies)} ä¸ªä»£ç†ï¼Œæœ€å¤šæµ‹è¯• {max_test_count} ä¸ª")
    
    # åˆå§‹åŒ–å¢å¼ºæµ‹è¯•å™¨
    tester = EnhancedLatencyTester(test_config)
    evaluator = tester.evaluator
    
    # æˆªå–å€™é€‰ä»£ç†
    candidates = proxies[:max_test_count]
    
    # æ‰¹é‡æµ‹è¯•ä»£ç†
    logger.info(f"å¹¶å‘æµ‹è¯• {len(candidates)} ä¸ªå€™é€‰ä»£ç†")
    test_results = tester.batch_test_proxies(candidates)
    
    # ç­›é€‰å¯ç”¨ä»£ç†
    working_proxies = [
        proxy for proxy in test_results
        if (
            proxy.get('is_working') and
            proxy.get('success_rate', 0) >= min_success_rate and
            proxy.get('avg_latency_ms') is not None and
            proxy.get('avg_latency_ms') <= max_latency_ms
        )
    ]
    
    if not working_proxies:
        logger.warning(f"âŒ åœ¨ {len(test_results)} ä¸ªæµ‹è¯•çš„ä»£ç†ä¸­æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä»£ç†")
        return None
    
    # ä½¿ç”¨è¯„ä¼°å™¨é€‰æ‹©æœ€ä½³ä»£ç†
    best_proxy = evaluator.select_best_proxy(
        working_proxies,
        min_composite_score,
        min_qos_score
    )
    
    if not best_proxy:
        logger.warning(f"âŒ æ²¡æœ‰ä»£ç†è¾¾åˆ°è¯„åˆ†è¦æ±‚ï¼ˆç»¼åˆ>={min_composite_score}, QoS>={min_qos_score}ï¼‰")
        return None
    
    # è¾“å‡ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    logger.info(f"ğŸ¯ é€‰æ‹©æœ€ä½³ä»£ç†: {best_proxy['proxy']}")
    
    # è¾“å‡ºåŸºç¡€æŒ‡æ ‡
    enhanced_stats = best_proxy.get('enhanced_stats', {})
    basic_stats = enhanced_stats.get('basic_stats', {})
    api_performance = enhanced_stats.get('api_performance', {})
    
    logger.info(f"   - å¹³å‡å»¶è¿Ÿ: {basic_stats.get('mean_latency_ms', 0):.1f}ms")
    logger.info(f"   - ä¸­ä½æ•°å»¶è¿Ÿ: {basic_stats.get('median_latency_ms', 0):.1f}ms")
    logger.info(f"   - P95å»¶è¿Ÿ: {basic_stats.get('p95_latency_ms', 0):.1f}ms")
    logger.info(f"   - æˆåŠŸç‡: {best_proxy.get('success_rate', 0):.1%}")
    logger.info(f"   - ç»¼åˆè¯„åˆ†: {best_proxy.get('composite_score', 0):.3f}")
    logger.info(f"   - QoSè¯„åˆ†: {best_proxy.get('qos_score', 0):.3f}")
    
    # è¾“å‡ºé«˜çº§æŒ‡æ ‡
    variability_stats = enhanced_stats.get('variability_stats', {})
    robustness_stats = enhanced_stats.get('robustness_stats', {})
    
    logger.info(f"   - å˜å¼‚ç³»æ•°: {variability_stats.get('coefficient_variation', 0):.3f}")
    logger.info(f"   - ä¸€è‡´æ€§è¯„åˆ†: {robustness_stats.get('consistency_score', 0):.3f}")
    logger.info(f"   - çªå‘å»¶è¿Ÿç‡: {api_performance.get('spike_rate', 0):.1%}")
    logger.info(f"   - è¶…æ—¶é£é™©: {api_performance.get('timeout_risk_score', 0):.3f}")
    
    # è¾“å‡ºæµ‹è¯•è¯¦æƒ…
    logger.info(f"   - æµ‹è¯•æ¬¡æ•°: {best_proxy.get('working_tests', 0)}/{best_proxy.get('test_count', 0)}")
    
    if best_proxy.get('is_fallback'):
        logger.info(f"   - ç±»å‹: é™çº§æµ‹è¯•ç»“æœ")
    
    return best_proxy


def get_cached_md5() -> Optional[str]:
    """
    è¯»å–ç¼“å­˜çš„MD5å€¼
    
    Returns:
        Optional[str]: ç¼“å­˜çš„MD5å€¼ï¼Œå¦‚æœç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥åˆ™è¿”å›None
    """
    try:
        if os.path.exists(PROXY_CACHE_FILE):
            with open(PROXY_CACHE_FILE, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                return cache_data.get('last_md5')
    except (json.JSONDecodeError, IOError) as e:
        logger.warning(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
    return None


def save_cached_md5(md5_value: str) -> None:
    """
    ä¿å­˜æ–°çš„MD5å€¼åˆ°ç¼“å­˜æ–‡ä»¶
    
    Args:
        md5_value: è¦ä¿å­˜çš„MD5å€¼
    """
    try:
        cache_data = {
            'last_md5': md5_value,
            'last_updated': datetime.now().isoformat()
        }
        with open(PROXY_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)
        logger.debug(f"å·²ä¿å­˜MD5ç¼“å­˜: {md5_value}")
    except IOError as e:
        logger.warning(f"ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")


def calculate_content_md5(content: str) -> str:
    """
    è®¡ç®—å†…å®¹çš„MD5å€¼
    
    Args:
        content: è¦è®¡ç®—MD5çš„å†…å®¹
    
    Returns:
        str: å†…å®¹çš„MD5å€¼
    """
    return hashlib.md5(content.encode('utf-8')).hexdigest()


def test_proxy_connectivity(proxy_ip: str, proxy_port: str, proxy_user: str, proxy_password: str = "1",
                          test_url: str = "http://www.gstatic.com/generate_204", timeout: int = 10) -> dict:
    """
    æµ‹è¯•ä»£ç†çš„è¿é€šæ€§å’Œå»¶è¿Ÿ
    
    Args:
        proxy_ip: ä»£ç†IPåœ°å€
        proxy_port: ä»£ç†ç«¯å£
        proxy_user: ä»£ç†ç”¨æˆ·å
        proxy_password: ä»£ç†å¯†ç  (é»˜è®¤ä¸º1)
        test_url: æµ‹è¯•URL (é»˜è®¤ http://www.gstatic.com/generate_204)
        timeout: è¶…æ—¶æ—¶é—´ (ç§’)
    
    Returns:
        dict: åŒ…å«è¿é€šæ€§å’Œå»¶è¿Ÿä¿¡æ¯çš„å­—å…¸
    """
    try:
        # æ„å»ºä»£ç†URLï¼ˆå¸¦è®¤è¯ä¿¡æ¯ï¼‰
        encoded_user = quote(proxy_user)
        proxy_url = f"http://{encoded_user}:{proxy_password}@{proxy_ip}:{proxy_port}"
        
        # è®¾ç½®ä»£ç†
        proxies = {
            'http': proxy_url,
            'https': proxy_url
        }
        
        # ä½¿ç”¨ User-Agentï¼›ä¸å†æ‰‹å·¥è®¾ç½® Proxy-Authorizationï¼Œäº¤ç»™ requests å¤„ç†
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        logger.debug(f"æ­£åœ¨æµ‹è¯•ä»£ç†: {proxy_ip}:{proxy_port}")
        
        # å‘é€æµ‹è¯•è¯·æ±‚ï¼ˆstream=Trueï¼Œä»…æµ‹åˆ°é¦–å­—èŠ‚/é¦–åŒ…æ—¶é—´ï¼‰
        response = requests.get(
            test_url,
            proxies=proxies,
            headers=headers,
            timeout=timeout,
            verify=False,
            stream=True,
            allow_redirects=False
        )
        latency_ms = response.elapsed.total_seconds() * 1000 if response.elapsed else None
        # å…³é—­è¿æ¥ï¼Œé¿å…ä¸‹è½½æ­£æ–‡
        response.close()

        if 200 <= response.status_code < 400 and latency_ms is not None:
            logger.debug(f"âœ… ä»£ç† {proxy_ip}:{proxy_port} æµ‹è¯•æˆåŠŸï¼ŒTTFB: {latency_ms:.1f}ms")
            return {
                'is_working': True,
                'latency_ms': latency_ms,
                'status_code': response.status_code,
                'test_url': test_url,
                'error': None
            }
        else:
            logger.debug(f"âŒ ä»£ç† {proxy_ip}:{proxy_port} æµ‹è¯•å¤±è´¥: HTTP {response.status_code}")
            return {
                'is_working': False,
                'latency_ms': latency_ms,
                'status_code': response.status_code,
                'test_url': test_url,
                'error': f"HTTP {response.status_code}"
            }
            
    except requests.exceptions.Timeout:
        logger.debug(f"âŒ ä»£ç† {proxy_ip}:{proxy_port} æµ‹è¯•è¶…æ—¶")
        return {
            'is_working': False,
            'latency_ms': None,
            'status_code': None,
            'test_url': test_url,
            'error': "Timeout"
        }
    except requests.exceptions.ProxyError:
        logger.debug(f"âŒ ä»£ç† {proxy_ip}:{proxy_port} ä»£ç†é”™è¯¯")
        return {
            'is_working': False,
            'latency_ms': None,
            'status_code': None,
            'test_url': test_url,
            'error': "Proxy Error"
        }
    except requests.exceptions.ConnectionError:
        logger.debug(f"âŒ ä»£ç† {proxy_ip}:{proxy_port} è¿æ¥é”™è¯¯")
        return {
            'is_working': False,
            'latency_ms': None,
            'status_code': None,
            'test_url': test_url,
            'error': "Connection Error"
        }
    except Exception as e:
        logger.debug(f"âŒ ä»£ç† {proxy_ip}:{proxy_port} æµ‹è¯•å¼‚å¸¸: {str(e)}")
        return {
            'is_working': False,
            'latency_ms': None,
            'status_code': None,
            'test_url': test_url,
            'error': str(e)
        }


def measure_proxy_latency(
    proxy_ip: str,
    proxy_port: str,
    proxy_user: str,
    proxy_password: str = "1",
    test_urls: List[str] = None,
    test_count: int = 3,
    timeout: int = 10,
) -> dict:
    """
    ç²¾ç¡®æµ‹é‡ä»£ç†å»¶è¿Ÿï¼Œä½¿ç”¨å¤šä¸ªURLå’Œå¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
    
    Args:
        proxy_ip: ä»£ç†IPåœ°å€
        proxy_port: ä»£ç†ç«¯å£
        proxy_user: ä»£ç†ç”¨æˆ·å
        proxy_password: ä»£ç†å¯†ç  (é»˜è®¤ä¸º1)
        test_urls: æµ‹è¯•URLåˆ—è¡¨ (é»˜è®¤ä½¿ç”¨å†…ç½®çš„å¿«é€Ÿå“åº”ç«™ç‚¹)
        test_count: æ¯ä¸ªURLæµ‹è¯•æ¬¡æ•° (é»˜è®¤ä¸º3)
        timeout: è¶…æ—¶æ—¶é—´ (ç§’)
    
    Returns:
        dict: åŒ…å«è¯¦ç»†å»¶è¿Ÿä¿¡æ¯çš„å­—å…¸
    """
    if test_urls is None:
        # é€‰æ‹©æå°å“åº”æˆ– 204 çš„æ¢æ´»ç«¯ç‚¹ï¼Œå‡å°‘ä¸‹è½½æ—¶é—´å½±å“
        test_urls = [
            "http://www.gstatic.com/generate_204",
            "https://www.gstatic.com/generate_204",
            "http://cp.cloudflare.com/generate_204",
        ]
    
    all_results = []
    working_tests = 0
    
    logger.debug(f"å¼€å§‹ç²¾ç¡®æµ‹é‡ä»£ç†å»¶è¿Ÿ: {proxy_ip}:{proxy_port}")
    
    test_details: List[Dict[str, Any]] = []

    for test_url in test_urls:
        url_results: List[Dict[str, Any]] = []

        for i in range(test_count):
            result = test_proxy_connectivity(
                proxy_ip, proxy_port, proxy_user, proxy_password, test_url, timeout
            )
            url_results.append(result)

            if result['is_working'] and result['latency_ms'] is not None:
                working_tests += 1
                all_results.append(result['latency_ms'])

            # æ·»åŠ çŸ­æš‚å»¶è¿Ÿé¿å…é¢‘ç¹è¯·æ±‚
            if i < test_count - 1:
                time.sleep(0.2)

        test_details.append({
            'url': test_url,
            'results': url_results,
        })
    
    if not working_tests:
        logger.debug(f"âŒ ä»£ç† {proxy_ip}:{proxy_port} æ‰€æœ‰æµ‹è¯•å‡å¤±è´¥")
        return {
            'proxy': f"{proxy_ip}:{proxy_port}",
            'user': proxy_user,
            'is_working': False,
            'avg_latency_ms': None,
            'min_latency_ms': None,
            'max_latency_ms': None,
            'success_rate': 0.0,
            'test_count': len(test_urls) * test_count,
            'working_tests': 0,
            'test_details': test_details,
        }
    
    # å»æ‰æœ€é«˜å’Œæœ€ä½å»¶è¿Ÿåè®¡ç®—å¹³å‡å€¼
    valid_latencies = [latency for latency in all_results if latency is not None]
    filtered_latencies: List[float] = []
    if len(valid_latencies) >= 5:
        valid_latencies.sort()
        trim = max(1, int(len(valid_latencies) * 0.1))
        filtered_latencies = valid_latencies[trim: len(valid_latencies) - trim]
    else:
        filtered_latencies = valid_latencies

    avg_latency = (
        sum(filtered_latencies) / len(filtered_latencies)
        if filtered_latencies else None
    )
    min_latency = min(valid_latencies) if valid_latencies else None
    max_latency = max(valid_latencies) if valid_latencies else None
    success_rate = working_tests / (len(test_urls) * test_count)
    
    if avg_latency is not None:
        logger.debug(
            f"âœ… ä»£ç† {proxy_ip}:{proxy_port} å»¶è¿Ÿæµ‹è¯•å®Œæˆ: å¹³å‡={avg_latency:.1f}ms, æˆåŠŸç‡={success_rate:.1%}"
        )
    else:
        logger.debug(
            f"âš ï¸ ä»£ç† {proxy_ip}:{proxy_port} å»¶è¿Ÿæµ‹è¯•å®Œæˆ: æ— æœ‰æ•ˆæ ·æœ¬, æˆåŠŸç‡={success_rate:.1%}"
        )
    
    return {
        'proxy': f"{proxy_ip}:{proxy_port}",
        'user': proxy_user,
        'is_working': True,
        'avg_latency_ms': avg_latency,
        'min_latency_ms': min_latency,
        'max_latency_ms': max_latency,
        'success_rate': success_rate,
        'test_count': len(test_urls) * test_count,
        'working_tests': working_tests,
        'test_details': test_details
    }


def _measure_single_proxy(
    ip_port: str,
    user: str,
    test_urls: Optional[List[str]],
    test_count: int,
    timeout: int,
) -> dict:
    try:
        proxy_ip, proxy_port = ip_port.split(":")
    except ValueError:
        return {
            'proxy': ip_port,
            'user': user,
            'is_working': False,
            'avg_latency_ms': None,
            'min_latency_ms': None,
            'max_latency_ms': None,
            'success_rate': 0.0,
            'test_count': 0,
            'working_tests': 0,
            'test_details': [],
            'error': 'Invalid ip:port'
        }

    return measure_proxy_latency(
        proxy_ip, proxy_port, user,
        test_urls=test_urls,
        test_count=test_count,
        timeout=timeout,
    )


def find_best_proxy_by_latency(
    proxies: List[Tuple[str, str]],
    max_test_count: int = 5,
    min_success_rate: float = 0.8,
    max_latency_ms: int = 5000,
    test_urls: Optional[List[str]] = None,
    test_count: int = 3,
    timeout: int = 10,
    concurrency: int = 5,
) -> Optional[dict]:
    """
    éå†ä»£ç†åˆ—è¡¨ï¼Œæ‰¾åˆ°å»¶è¿Ÿæœ€ä½çš„å¯ç”¨ä»£ç†
    
    Args:
        proxies: ä»£ç†åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º(ip_port, user)å…ƒç»„
        max_test_count: æœ€å¤§æµ‹è¯•æ•°é‡ (é»˜è®¤ä¸º5)
        min_success_rate: æœ€ä½æˆåŠŸç‡è¦æ±‚ (é»˜è®¤ä¸º0.8)
        max_latency_ms: æœ€å¤§å¯æ¥å—å»¶è¿Ÿ (é»˜è®¤ä¸º5000ms)
    
    Returns:
        Optional[dict]: æœ€ä½³ä»£ç†çš„è¯¦ç»†ä¿¡æ¯ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
    """
    logger.info(f"å¼€å§‹æµ‹è¯•ä»£ç†å»¶è¿Ÿï¼Œå…± {len(proxies)} ä¸ªä»£ç†ï¼Œæœ€å¤šæµ‹è¯• {max_test_count} ä¸ª")
    
    tested_proxies = []

    # æˆªå–åˆ°æœ€å¤š max_test_count ä¸ªå€™é€‰
    candidates = proxies[:max_test_count]

    logger.info(f"å¹¶å‘æµ‹è¯• {len(candidates)} ä¸ªå€™é€‰ä»£ç†ï¼Œæœ€å¤§å¹¶å‘={concurrency}")

    with ThreadPoolExecutor(max_workers=max(1, concurrency)) as executor:
        future_map = {
            executor.submit(
                _measure_single_proxy,
                ip_port,
                user,
                test_urls,
                test_count,
                timeout,
            ): (ip_port, user)
            for (ip_port, user) in candidates
        }

        for future in as_completed(future_map):
            ip_port, user = future_map[future]
            try:
                result = future.result()
            except Exception as e:
                logger.warning(f"ä»£ç† {ip_port} æµ‹è¯•å¼‚å¸¸: {e}")
                result = {
                    'proxy': ip_port,
                    'user': user,
                    'is_working': False,
                    'avg_latency_ms': None,
                    'min_latency_ms': None,
                    'max_latency_ms': None,
                    'success_rate': 0.0,
                    'test_count': 0,
                    'working_tests': 0,
                    'test_details': [],
                    'error': str(e)
                }

            tested_proxies.append(result)
            if result['is_working'] and result['avg_latency_ms'] is not None:
                logger.info(
                    f"âœ… ä»£ç† {ip_port} æµ‹è¯•å®Œæˆ: å¹³å‡å»¶è¿Ÿ={result['avg_latency_ms']:.1f}ms, æˆåŠŸç‡={result['success_rate']:.1%}"
                )
            else:
                logger.info(f"âŒ ä»£ç† {ip_port} ä¸å¯ç”¨")
    
    # ç­›é€‰å¯ç”¨ä»£ç†
    working_proxies = [
        proxy for proxy in tested_proxies
        if (
            proxy.get('is_working')
            and proxy.get('success_rate', 0) >= min_success_rate
            and proxy.get('avg_latency_ms') is not None
            and proxy.get('avg_latency_ms') <= max_latency_ms
        )
    ]
    
    if not working_proxies:
        logger.warning(f"âŒ åœ¨ {len(tested_proxies)} ä¸ªæµ‹è¯•çš„ä»£ç†ä¸­æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä»£ç†")
        return None
    
    # æŒ‰å»¶è¿Ÿæ’åºï¼Œé€‰æ‹©æœ€ä½³ä»£ç†
    best_proxy = min(working_proxies, key=lambda x: x['avg_latency_ms'])
    
    logger.info(f"ğŸ¯ é€‰æ‹©æœ€ä½³ä»£ç†: {best_proxy['proxy']}")
    logger.info(f"   - å¹³å‡å»¶è¿Ÿ: {best_proxy['avg_latency_ms']:.1f}ms")
    logger.info(f"   - æœ€å°å»¶è¿Ÿ: {best_proxy['min_latency_ms']:.1f}ms")
    logger.info(f"   - æœ€å¤§å»¶è¿Ÿ: {best_proxy['max_latency_ms']:.1f}ms")
    logger.info(f"   - æˆåŠŸç‡: {best_proxy['success_rate']:.1%}")
    logger.info(f"   - æµ‹è¯•æ¬¡æ•°: {best_proxy['working_tests']}/{best_proxy['test_count']}")
    
    return best_proxy


def extract_proxies_by_region(markdown_text: str, region: str) -> List[Tuple[str, str]]:
    """
    ä»Markdownæ–‡æœ¬ä¸­æå–æŒ‡å®šåœ°åŒºçš„ä»£ç†
    
    Args:
        markdown_text: åŒ…å«ä»£ç†åˆ—è¡¨çš„Markdownæ–‡æœ¬
        region: ç›®æ ‡åœ°åŒº
    
    Returns:
        åŒ…å«(ip_port, user)å…ƒç»„çš„åˆ—è¡¨
    """
    proxies: List[Tuple[str, str]] = []
    lines = markdown_text.split('\n')
    ip_port_regex = re.compile(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+$')
    
    for line in lines:
        # æ£€æŸ¥æ˜¯å¦ä¸ºè¡¨æ ¼æ•°æ®è¡Œ
        if line.startswith('|') and not line.startswith('|---'):
            # åˆ†å‰²åˆ—å¹¶å»é™¤é¦–å°¾ç©ºæ ¼
            columns = [col.strip() for col in line.split('|') if col.strip()]
            
            if len(columns) >= 3:
                ip_port = columns[0]
                country = columns[1]
                user = columns[2]
                
                # ç­›é€‰æŒ‡å®šåœ°åŒºä¸”IP:ç«¯å£æ ¼å¼æ­£ç¡®çš„ä»£ç†
                if country == region and ip_port_regex.match(ip_port):
                    proxies.append((ip_port, user))
    
    return proxies


def update_channel_proxy(base_url: str, admin_id: str, admin_token: str, channel_ids: List[int], proxy_url: str) -> None:
    """
    æ›´æ–°æŒ‡å®šæ¸ é“çš„ä»£ç†è®¾ç½®
    
    Args:
        base_url: New-APIåŸºç¡€URL
        admin_id: ç®¡ç†å‘˜ID
        admin_token: ç®¡ç†å‘˜ä»¤ç‰Œ
        channel_ids: æ¸ é“IDåˆ—è¡¨
        proxy_url: æ–°çš„ä»£ç†URL
    """
    api_url = f"{base_url.rstrip('/')}/api/channel/"
    
    logger.info(f"æ­£åœ¨æ›´æ–° {len(channel_ids)} ä¸ªæ¸ é“çš„ä»£ç†...")
    logger.debug(f"updateChannelProxy: proxyUrl={proxy_url}")
    logger.debug(f"updateChannelProxy: channelIds={channel_ids}")
    logger.debug(f"updateChannelProxy: baseUrl={base_url}")
    
    headers = {
        'New-Api-User': admin_id,
        'Authorization': f'Bearer {admin_token}',
        'Content-Type': 'application/json',
    }
    
    for channel_id in channel_ids:
        update_data = {
            'id': channel_id,
            'setting': json.dumps({'proxy': proxy_url}),
        }
        
        logger.info(f"æ­£åœ¨æ›´æ–°æ¸ é“ {channel_id} çš„ä»£ç†...")
        logger.debug(f"updateChannelProxy: updateData={json.dumps(update_data)}")
        logger.debug(f"updateChannelProxy: headers.New-Api-User={admin_id}")
        logger.debug(f"updateChannelProxy: headers.Authorization=Bearer {admin_token[:10]}...")
        logger.debug(f"updateChannelProxy: body={json.dumps(update_data)}")
        
        try:
            response = requests.put(
                api_url,
                headers=headers,
                json=update_data,
                timeout=30
            )
            
            if response.ok:
                logger.info(f"âœ… æ¸ é“ {channel_id} ä»£ç†æ›´æ–°æˆåŠŸï¼")
            else:
                error_text = response.text
                logger.error(f"âŒ æ¸ é“ {channel_id} ä»£ç†æ›´æ–°å¤±è´¥ï¼šHTTP {response.status_code}: {error_text}")
                raise Exception(f"ä»£ç†æ›´æ–°å¤±è´¥ï¼šHTTP {response.status_code}: {error_text}")
                
        except requests.RequestException as e:
            logger.error(f"âŒ æ›´æ–°æ¸ é“ {channel_id} æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯ï¼š{str(e)}")
            raise


def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œä»£ç†æ›´æ–°ä»»åŠ¡"""
    logger.info("å¼€å§‹æ‰§è¡Œä»£ç†æ›´æ–°ä»»åŠ¡...")
    
    # æ­¥éª¤1: æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_env_vars = {
        'BASE_URL': os.getenv('BASE_URL'),
        'ADMIN_ID': os.getenv('ADMIN_ID'),
        'ADMIN_TOKEN': os.getenv('ADMIN_TOKEN'),
        'CHANNEL_IDS': os.getenv('CHANNEL_IDS')
    }
    
    missing_vars = [var for var, value in required_env_vars.items() if not value]
    if missing_vars:
        logger.error(f"é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        sys.exit(1)
    
    try:
        # è§£ææ¸ é“ID
        channel_ids = json.loads(required_env_vars['CHANNEL_IDS'])
        if not isinstance(channel_ids, list):
            raise ValueError("CHANNEL_IDS å¿…é¡»æ˜¯æ•°ç»„æ ¼å¼")
        
        # æ­¥éª¤2: ä»GitHubè·å–ä»£ç†åˆ—è¡¨
        logger.info("æ­£åœ¨ä»GitHubè·å–ä»£ç†åˆ—è¡¨...")
        proxy_list_url = "https://raw.githubusercontent.com/TopChina/proxy-list/refs/heads/main/README.md"
        
        try:
            response = requests.get(proxy_list_url, timeout=30)
            response.raise_for_status()
            markdown_text = response.text
            logger.info("âœ… æˆåŠŸè·å–ä»£ç†åˆ—è¡¨Markdownæ–‡ä»¶")
            
            # æ£€æŸ¥å†…å®¹MD5æ˜¯å¦ä¸ç¼“å­˜ç›¸åŒ
            current_md5 = calculate_content_md5(markdown_text)
            cached_md5 = get_cached_md5()
            
            if cached_md5 == current_md5:
                logger.info("ğŸ”„ ä»£ç†åˆ—è¡¨å†…å®¹æœªå˜åŒ–ï¼Œè·³è¿‡æœ¬æ¬¡æ‰§è¡Œ")
                return
            else:
                logger.info("ğŸ“ ä»£ç†åˆ—è¡¨å†…å®¹æœ‰æ›´æ–°ï¼Œç»§ç»­æ‰§è¡Œ")
        except requests.RequestException as e:
            logger.error(f"âŒ è·å–ä»£ç†åˆ—è¡¨å¤±è´¥ï¼š{str(e)}")
            return
        
        # æ­¥éª¤3: è§£æä»£ç†åˆ—è¡¨
        region = os.getenv('PROXY_REGION', 'é¦™æ¸¯')
        logger.info(f"æ­£åœ¨æŸ¥æ‰¾ {region} åœ°åŒºçš„ä»£ç†...")
        
        proxies = extract_proxies_by_region(markdown_text, region)
        
        if not proxies:
            logger.warning(f"åœ¨åˆ—è¡¨ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„ {region} ä»£ç†ã€‚ä»»åŠ¡ç»“æŸã€‚")
            return
        
        logger.info(f"âœ… æ‰¾åˆ°äº† {len(proxies)} ä¸ª {region} ä»£ç†")
        
        # æ­¥éª¤4: æµ‹è¯•ä»£ç†å»¶è¿Ÿå¹¶æ‰¾åˆ°æœ€ä½³ä»£ç†ï¼ˆæ”¯æŒå‚æ•°è°ƒä¼˜ï¼‰
        max_test_count = int(os.getenv('MAX_PROXY_TEST_COUNT', '5'))
        min_success_rate = float(os.getenv('MIN_SUCCESS_RATE', '0.8'))
        max_latency_ms = int(os.getenv('MAX_LATENCY_MS', '5000'))
        test_count = int(os.getenv('TEST_COUNT', '3'))
        timeout = int(os.getenv('TEST_TIMEOUT', '10'))
        concurrency = int(os.getenv('TEST_CONCURRENCY', '5'))

        # å¯é€‰åœ°å…è®¸è‡ªå®šä¹‰æµ‹è¯• URL åˆ—è¡¨ï¼ˆä»¥é€—å·åˆ†éš”ï¼‰
        custom_test_urls = os.getenv('TEST_URLS')
        test_urls = None
        if custom_test_urls:
            test_urls = [u.strip() for u in custom_test_urls.split(',') if u.strip()]
        
        logger.info("å¼€å§‹æµ‹è¯•ä»£ç†å»¶è¿Ÿï¼Œé€‰æ‹©æœ€ä½³ä»£ç†...")
        best_proxy_info = find_best_proxy_by_latency_enhanced(
            proxies,
            max_test_count=max_test_count,
        )
        
        if not best_proxy_info:
            logger.error("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä»£ç†ï¼Œä»»åŠ¡ç»“æŸã€‚")
            return
        
        ip_port = best_proxy_info['proxy']
        user = best_proxy_info['user']
        host, port = ip_port.split(':')
        password = "1"
        
        # å¯¹ç”¨æˆ·åè¿›è¡ŒURLç¼–ç ä»¥å¤„ç†ç‰¹æ®Šå­—ç¬¦
        from urllib.parse import quote
        encoded_user = quote(user)
        proxy_url = f"http://{encoded_user}:{password}@{host}:{port}"
        
        logger.info(f"âœ… é€‰æ‹©æœ€ä½³ä»£ç†ï¼Œå‡†å¤‡ä½¿ç”¨ï¼š{proxy_url}")
        logger.info(f"ğŸ“Š ä»£ç†æ€§èƒ½ç»Ÿè®¡ï¼š")
        logger.info(f"   - å¹³å‡å»¶è¿Ÿ: {best_proxy_info['avg_latency_ms']:.1f}ms")
        logger.info(f"   - æˆåŠŸç‡: {best_proxy_info['success_rate']:.1%}")
        logger.info(f"   - æµ‹è¯•æ¬¡æ•°: {best_proxy_info['working_tests']}/{best_proxy_info['test_count']}")
        
        # æ­¥éª¤5: æ›´æ–°æ¸ é“ä»£ç†
        update_channel_proxy(
            base_url=required_env_vars['BASE_URL'],
            admin_id=required_env_vars['ADMIN_ID'],
            admin_token=required_env_vars['ADMIN_TOKEN'],
            channel_ids=channel_ids,
            proxy_url=proxy_url
        )
        
        # ä¿å­˜æ–°çš„MD5å€¼åˆ°ç¼“å­˜
        save_cached_md5(current_md5)
        logger.info("âœ… ä»£ç†æ›´æ–°ä»»åŠ¡å®Œæˆï¼")
        
    except json.JSONDecodeError as e:
        logger.error(f"âŒ JSONè§£æé”™è¯¯ï¼š{str(e)}")
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼š{str(e)}")


def run_scheduled_task():
    """å®šæ—¶ä»»åŠ¡å…¥å£"""
    logger.info(f"Cronä»»åŠ¡è§¦å‘ï¼å¼€å§‹æ‰§è¡Œï¼š{datetime.now()}")
    main()


if __name__ == "__main__":
    main()
